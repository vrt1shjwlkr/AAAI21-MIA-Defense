{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook contains code for \n",
    "\n",
    "### Dataset partitions\n",
    "We require following partitions of the dataset:\n",
    "- Private training data (size=10,000), \n",
    "- Reference data (size=10,000), \n",
    "- Test data (size=5,000), \n",
    "- Members to train the membership inference attack (MIA) model: 5,000 samples from the private training data\n",
    "- Non-members to train MIA model: 5,000 samples that are not used in any of the above dataset partition\n",
    "\n",
    "\n",
    "### DMP training\n",
    "Specifically, we train an unprotected model on the private data, then distill the knowledge of the unprotected model in to randomly sampled reference data, use the distilled knowledge to train the final protected model\n",
    "\n",
    "### A blackbox membership inference attack that evaluates MIA risk of the DMP models\n",
    "Although there are multiple blackbox MIAs in the literature, we use the one proposed in \"Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning\". We do not give the code for whitebox MIAs, as it is observed in many works (including ours) that whitebox MIAs are not really stronger than blackbox MIAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from purchase_models import *\n",
    "from purchase_normal_train import *\n",
    "from purchase_private_train import *\n",
    "from purchase_attack_train import *\n",
    "from purchase_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len:  197324\n",
      "tr len 10000 | mia_members tr 5000 val 2500 te 2500 | mia_nonmembers tr 5000 val 2500 te 2500 | ref len 10000 | val len 5000 | test len 5000 | attack te len 10000 | remaining data len 147324\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "data_loc='/mnt/nfs/work1/amir/vshejwalkar/purchase_data/dataset_purchase'\n",
    "private_data_len=10000\n",
    "ref_data_len=10000\n",
    "te_len=5000\n",
    "val_len=5000\n",
    "attack_tr_len=10000\n",
    "attack_te_len=10000\n",
    "\n",
    "\n",
    "tr_frac=0.5 # we use 50% of private data as the members to train MIA model\n",
    "val_frac=0.25 # we use 25% of private data as the members to validate MIA model\n",
    "te_frac=0.25 # we use 25% of private data as the members to test MIA model\n",
    "\n",
    "data_set = np.genfromtxt(data_loc, delimiter=',')\n",
    "X = data_set[:,1:].astype(np.float64)\n",
    "Y = (data_set[:,0]).astype(np.int32)-1\n",
    "\n",
    "print('total data len: ',len(X))\n",
    "\n",
    "if not os.path.isfile('./purchase_shuffle.pkl'):\n",
    "    all_indices = np.arange(len(X))\n",
    "    np.random.shuffle(all_indices)\n",
    "    pickle.dump(all_indices,open('./purchase_shuffle.pkl','wb'))\n",
    "else:\n",
    "    all_indices=pickle.load(open('./purchase_shuffle.pkl','rb'))\n",
    "\n",
    "\n",
    "private_data=X[all_indices[:private_data_len]]\n",
    "private_label=Y[all_indices[:private_data_len]]\n",
    "\n",
    "ref_data=X[all_indices[private_data_len : (private_data_len + ref_data_len)]]\n",
    "ref_label=Y[all_indices[private_data_len : (private_data_len + ref_data_len)]]\n",
    "\n",
    "val_data=X[all_indices[(private_data_len + ref_data_len):(private_data_len + ref_data_len + val_len)]]\n",
    "val_label=Y[all_indices[(private_data_len + ref_data_len):(private_data_len + ref_data_len + val_len)]]\n",
    "\n",
    "te_data=X[all_indices[(private_data_len + ref_data_len + val_len):(private_data_len + ref_data_len + val_len + te_len)]]\n",
    "te_label=Y[all_indices[(private_data_len + ref_data_len + val_len):(private_data_len + ref_data_len + val_len + te_len)]]\n",
    "\n",
    "attack_te_data=X[all_indices[(private_data_len + ref_data_len + val_len+ te_len):(private_data_len + ref_data_len + val_len+ te_len + attack_te_len)]]\n",
    "attack_te_label=Y[all_indices[(private_data_len + ref_data_len + val_len+ te_len):(private_data_len + ref_data_len + val_len+ te_len + attack_te_len)]]\n",
    "\n",
    "attack_tr_data=X[all_indices[(private_data_len + ref_data_len + val_len+ te_len + attack_te_len):(private_data_len + ref_data_len + val_len+ te_len + attack_te_len + attack_tr_len)]]\n",
    "attack_tr_label=Y[all_indices[(private_data_len + ref_data_len + val_len+ te_len + attack_te_len):(private_data_len + ref_data_len + val_len+ te_len + attack_te_len + attack_tr_len)]]\n",
    "\n",
    "remaining_data=X[all_indices[(private_data_len + ref_data_len + val_len+ te_len + attack_te_len + attack_tr_len):]]\n",
    "remaining_label=Y[all_indices[(private_data_len + ref_data_len + val_len+ te_len + attack_te_len + attack_tr_len):]]\n",
    "\n",
    "\n",
    "# get private data and label tensors required to train the unprotected model\n",
    "private_data_tensor=torch.from_numpy(private_data).type(torch.FloatTensor)\n",
    "private_label_tensor=torch.from_numpy(private_label).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "# get reference data and label tensors required to distil the knowledge into the protected model\n",
    "ref_indices=np.arange((ref_data_len))\n",
    "ref_data_tensor=torch.from_numpy(ref_data).type(torch.FloatTensor)\n",
    "ref_label_tensor=torch.from_numpy(ref_label).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "## Member tensors required to train, validate, and test the MIA model:\n",
    "# get member data and label tensors required to train MIA model\n",
    "mia_train_members_data_tensor=private_data_tensor[:int(tr_frac*private_data_len)]\n",
    "mia_train_members_label_tensor=private_label_tensor[:int(tr_frac*private_data_len)]\n",
    "\n",
    "# get member data and label tensors required to validate MIA model\n",
    "mia_val_members_data_tensor=private_data_tensor[int(tr_frac*private_data_len):int((tr_frac+val_frac)*private_data_len)]\n",
    "mia_val_members_label_tensor=private_label_tensor[int(tr_frac*private_data_len):int((tr_frac+val_frac)*private_data_len)]\n",
    "\n",
    "# get member data and label tensors required to test MIA model\n",
    "mia_test_members_data_tensor=private_data_tensor[int((tr_frac+val_frac)*private_data_len):]\n",
    "mia_test_members_label_tensor=private_label_tensor[int((tr_frac+val_frac)*private_data_len):]\n",
    "\n",
    "\n",
    "\n",
    "## Non-member tensors required to train, validate, and test the MIA model:\n",
    "attack_tr_data_tensors = torch.from_numpy(attack_tr_data).type(torch.FloatTensor)\n",
    "attack_tr_label_tensors = torch.from_numpy(attack_tr_label).type(torch.LongTensor)\n",
    "\n",
    "# get non-member data and label tensors required to train \n",
    "mia_train_nonmembers_data_tensor = attack_tr_data_tensors[:int(tr_frac*private_data_len)]\n",
    "mia_train_nonmembers_label_tensor = attack_tr_label_tensors[:int(tr_frac*private_data_len)]\n",
    "\n",
    "# get member data and label tensors required to validate MIA model\n",
    "mia_val_nonmembers_data_tensor = attack_tr_data_tensors[int(tr_frac*private_data_len):int((tr_frac+val_frac)*private_data_len)]\n",
    "mia_val_nonmembers_label_tensor = attack_tr_label_tensors[int(tr_frac*private_data_len):int((tr_frac+val_frac)*private_data_len)]\n",
    "\n",
    "# get member data and label tensors required to test MIA model\n",
    "mia_test_nonmembers_data_tensor = attack_tr_data_tensors[int((tr_frac+val_frac)*private_data_len):]\n",
    "mia_test_nonmembers_label_tensor = attack_tr_label_tensors[int((tr_frac+val_frac)*private_data_len):]\n",
    "\n",
    "\n",
    "\n",
    "# get non-member data and label tensors required to test the MIA model\n",
    "attack_te_data_tensor=torch.from_numpy(attack_te_data).type(torch.FloatTensor)\n",
    "attack_te_label_tensor=torch.from_numpy(attack_te_label).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "## Tensors required to validate and test the unprotected and protected models\n",
    "# get validation data and label tensors\n",
    "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
    "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
    "\n",
    "# get test data and label tensors\n",
    "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
    "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "print('tr len %d | mia_members tr %d val %d te %d | mia_nonmembers tr %d val %d te %d | ref len %d | val len %d | test len %d | attack te len %d | remaining data len %d'%\n",
    "      (len(private_data_tensor),len(mia_train_members_data_tensor),len(mia_val_members_data_tensor),len(mia_test_members_data_tensor),\n",
    "       len(mia_train_nonmembers_data_tensor), len(mia_val_nonmembers_data_tensor),len(mia_test_nonmembers_data_tensor),\n",
    "       len(ref_data_tensor),len(val_data_tensor),len(te_data_tensor),len(attack_te_data_tensor), len(remaining_data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurchaseClassifier(nn.Module):\n",
    "    def __init__(self,num_classes=100):\n",
    "        super(PurchaseClassifier, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(600,1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(128,num_classes)\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        \n",
    "        outputs=[]\n",
    "        x=inp\n",
    "        module_list =list(self.features.modules())[1:]\n",
    "        for l in module_list:\n",
    "            \n",
    "            x = l(x)\n",
    "            outputs.append(x)\n",
    "        \n",
    "        y = x.view(inp.size(0), -1)\n",
    "        o = self.classifier(y)\n",
    "        \n",
    "        return o, outputs[-1].view(inp.size(0), -1), outputs[-4].view(inp.size(0), -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the size of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1317348"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=PurchaseClassifier()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pub(train_data, labels, true_labels, model, t_softmax, optimizer, num_batchs=999999, batch_size=16, alpha=1):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    true_criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "    len_t = len(train_data)//batch_size\n",
    "    if len(train_data) % batch_size:\n",
    "        len_t += 1\n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        if ind > num_batchs:\n",
    "            break\n",
    "\n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        true_targets=true_labels[ind*batch_size:(ind+1)*batch_size]\n",
    "\n",
    "\n",
    "        inputs, targets, true_targets = inputs.cuda(), targets.cuda(), true_targets.cuda()\n",
    "        \n",
    "        inputs, targets, true_targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets), torch.autograd.Variable(true_targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs, _, _ = model(inputs)\n",
    "\n",
    "        loss = alpha*F.kl_div(F.log_softmax(outputs/t_softmax, dim=1), F.softmax(targets/t_softmax, dim=1)) + (1-alpha)*true_criterion(outputs,true_targets)\n",
    "        \n",
    "        # measure loss\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data,labels,model,criterion,optimizer,epoch,use_cuda,num_batchs=999999,batch_size=32, uniform_reg=False):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    len_t = len(train_data)//batch_size\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "    \n",
    "    \n",
    "    for ind in range(len_t):\n",
    "        if ind > num_batchs:\n",
    "            break\n",
    "        \n",
    "        inputs = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        targets = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs,_,_ = model(inputs)\n",
    "        \n",
    "        if uniform_reg==True:\n",
    "            uniform_ = (torch.ones(batch_size,outputs.shape[1])).cuda()\n",
    "            t_softmax=1\n",
    "            loss = criterion(outputs, targets)+100*F.kl_div(F.log_softmax(outputs/t_softmax, dim=1), F.softmax(uniform_/t_softmax, dim=1))\n",
    "        else:\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lr=0.0005\n",
    "at_lr=0.0001\n",
    "n_classes=100\n",
    "t_softmax=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | tr acc 27.58 loss 2.96 | val acc 48.75 loss 2.00 | best val acc 48.75 | best te acc 47.95\n",
      "epoch 1 | tr acc 53.47 loss 1.67 | val acc 53.70 loss 1.56 | best val acc 53.70 | best te acc 51.95\n",
      "epoch 2 | tr acc 61.91 loss 1.28 | val acc 54.06 loss 1.42 | best val acc 54.06 | best te acc 52.95\n",
      "epoch 3 | tr acc 66.88 loss 1.08 | val acc 57.09 loss 1.30 | best val acc 57.09 | best te acc 56.17\n",
      "epoch 4 | tr acc 70.52 loss 0.93 | val acc 57.74 loss 1.29 | best val acc 57.74 | best te acc 57.38\n",
      "epoch 5 | tr acc 72.95 loss 0.85 | val acc 60.91 loss 1.18 | best val acc 60.91 | best te acc 59.16\n",
      "epoch 6 | tr acc 74.91 loss 0.78 | val acc 62.14 loss 1.14 | best val acc 62.14 | best te acc 60.55\n",
      "epoch 7 | tr acc 77.92 loss 0.70 | val acc 60.91 loss 1.18 | best val acc 62.14 | best te acc 60.55\n",
      "epoch 8 | tr acc 79.00 loss 0.65 | val acc 65.19 loss 1.03 | best val acc 65.19 | best te acc 64.57\n",
      "epoch 9 | tr acc 83.08 loss 0.55 | val acc 67.24 loss 0.95 | best val acc 67.24 | best te acc 66.78\n",
      "epoch 10 | tr acc 85.91 loss 0.48 | val acc 67.46 loss 0.95 | best val acc 67.46 | best te acc 66.72\n",
      "epoch 11 | tr acc 86.47 loss 0.45 | val acc 67.52 loss 0.95 | best val acc 67.52 | best te acc 67.52\n",
      "epoch 12 | tr acc 87.28 loss 0.43 | val acc 65.45 loss 1.08 | best val acc 67.52 | best te acc 67.52\n",
      "epoch 13 | tr acc 88.07 loss 0.40 | val acc 67.40 loss 0.98 | best val acc 67.52 | best te acc 67.52\n",
      "epoch 14 | tr acc 89.29 loss 0.37 | val acc 71.24 loss 0.87 | best val acc 71.24 | best te acc 69.55\n",
      "epoch 15 | tr acc 89.67 loss 0.36 | val acc 69.23 loss 0.96 | best val acc 71.24 | best te acc 69.55\n",
      "epoch 16 | tr acc 91.09 loss 0.32 | val acc 67.89 loss 1.03 | best val acc 71.24 | best te acc 69.55\n",
      "epoch 17 | tr acc 91.32 loss 0.31 | val acc 70.74 loss 0.90 | best val acc 71.24 | best te acc 69.55\n",
      "epoch 18 | tr acc 92.76 loss 0.28 | val acc 67.08 loss 1.04 | best val acc 71.24 | best te acc 69.55\n",
      "epoch 19 | tr acc 91.50 loss 0.30 | val acc 72.31 loss 0.83 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 20 | tr acc 92.85 loss 0.26 | val acc 70.14 loss 0.93 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 21 | tr acc 94.03 loss 0.23 | val acc 71.72 loss 0.88 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 22 | tr acc 94.10 loss 0.23 | val acc 71.84 loss 0.88 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 23 | tr acc 94.90 loss 0.21 | val acc 71.38 loss 0.88 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 24 | tr acc 94.84 loss 0.21 | val acc 71.52 loss 0.88 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 25 | tr acc 94.61 loss 0.22 | val acc 71.24 loss 0.91 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 26 | tr acc 95.84 loss 0.19 | val acc 69.05 loss 0.99 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 27 | tr acc 96.49 loss 0.17 | val acc 69.65 loss 0.97 | best val acc 72.31 | best te acc 72.15\n",
      "epoch 28 | tr acc 96.05 loss 0.18 | val acc 72.31 loss 0.87 | best val acc 72.31 | best te acc 71.24\n",
      "epoch 29 | tr acc 95.49 loss 0.19 | val acc 68.97 loss 0.98 | best val acc 72.31 | best te acc 71.24\n",
      "epoch 30 | tr acc 96.28 loss 0.17 | val acc 69.29 loss 1.02 | best val acc 72.31 | best te acc 71.24\n",
      "epoch 31 | tr acc 95.96 loss 0.17 | val acc 72.35 loss 0.87 | best val acc 72.35 | best te acc 71.97\n",
      "epoch 32 | tr acc 96.26 loss 0.17 | val acc 71.84 loss 0.87 | best val acc 72.35 | best te acc 71.97\n",
      "epoch 33 | tr acc 96.03 loss 0.17 | val acc 71.52 loss 0.90 | best val acc 72.35 | best te acc 71.97\n",
      "epoch 34 | tr acc 96.39 loss 0.16 | val acc 72.69 loss 0.87 | best val acc 72.69 | best te acc 71.86\n",
      "epoch 35 | tr acc 97.13 loss 0.15 | val acc 73.65 loss 0.81 | best val acc 73.65 | best te acc 73.39\n",
      "epoch 36 | tr acc 96.62 loss 0.16 | val acc 73.93 loss 0.80 | best val acc 73.93 | best te acc 72.75\n",
      "epoch 37 | tr acc 97.65 loss 0.14 | val acc 73.95 loss 0.80 | best val acc 73.95 | best te acc 73.05\n",
      "epoch 38 | tr acc 97.12 loss 0.15 | val acc 74.10 loss 0.81 | best val acc 74.10 | best te acc 73.37\n",
      "epoch 39 | tr acc 97.03 loss 0.15 | val acc 70.74 loss 0.94 | best val acc 74.10 | best te acc 73.37\n",
      "epoch 40 | tr acc 97.88 loss 0.13 | val acc 75.14 loss 0.78 | best val acc 75.14 | best te acc 74.26\n",
      "epoch 41 | tr acc 97.12 loss 0.15 | val acc 73.29 loss 0.84 | best val acc 75.14 | best te acc 74.26\n",
      "epoch 42 | tr acc 97.89 loss 0.13 | val acc 74.90 loss 0.74 | best val acc 75.14 | best te acc 74.26\n",
      "epoch 43 | tr acc 98.87 loss 0.10 | val acc 75.20 loss 0.75 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 44 | tr acc 98.83 loss 0.10 | val acc 73.37 loss 0.81 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 45 | tr acc 97.04 loss 0.15 | val acc 70.86 loss 0.95 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 46 | tr acc 96.91 loss 0.15 | val acc 74.30 loss 0.78 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 47 | tr acc 96.75 loss 0.15 | val acc 74.66 loss 0.80 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 48 | tr acc 98.33 loss 0.11 | val acc 74.26 loss 0.80 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 49 | tr acc 98.71 loss 0.10 | val acc 74.82 loss 0.76 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 50 | tr acc 99.13 loss 0.09 | val acc 72.71 loss 0.87 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 51 | tr acc 98.80 loss 0.11 | val acc 73.41 loss 0.84 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 52 | tr acc 97.61 loss 0.13 | val acc 71.97 loss 0.87 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 53 | tr acc 97.71 loss 0.13 | val acc 71.80 loss 0.88 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 54 | tr acc 97.10 loss 0.14 | val acc 74.04 loss 0.82 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 55 | tr acc 98.56 loss 0.11 | val acc 72.97 loss 0.88 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 56 | tr acc 99.48 loss 0.08 | val acc 74.94 loss 0.75 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 57 | tr acc 99.25 loss 0.08 | val acc 74.96 loss 0.78 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 58 | tr acc 99.56 loss 0.08 | val acc 74.86 loss 0.78 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 59 | tr acc 99.40 loss 0.08 | val acc 72.73 loss 0.84 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 60 | tr acc 94.61 loss 0.22 | val acc 69.63 loss 0.96 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 61 | tr acc 96.98 loss 0.14 | val acc 74.38 loss 0.78 | best val acc 75.20 | best te acc 74.72\n",
      "epoch 62 | tr acc 99.01 loss 0.09 | val acc 75.70 loss 0.75 | best val acc 75.70 | best te acc 75.22\n",
      "epoch 63 | tr acc 99.79 loss 0.07 | val acc 76.00 loss 0.72 | best val acc 76.00 | best te acc 75.82\n",
      "epoch 64 | tr acc 99.92 loss 0.06 | val acc 75.72 loss 0.71 | best val acc 76.00 | best te acc 75.82\n",
      "epoch 65 | tr acc 99.98 loss 0.06 | val acc 75.24 loss 0.72 | best val acc 76.00 | best te acc 75.82\n",
      "epoch 66 | tr acc 100.00 loss 0.06 | val acc 76.57 loss 0.70 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 67 | tr acc 100.00 loss 0.07 | val acc 75.46 loss 0.72 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 68 | tr acc 91.36 loss 0.31 | val acc 69.86 loss 0.95 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 69 | tr acc 95.00 loss 0.19 | val acc 73.19 loss 0.84 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 70 | tr acc 98.39 loss 0.10 | val acc 73.57 loss 0.82 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 71 | tr acc 99.38 loss 0.08 | val acc 75.60 loss 0.73 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 72 | tr acc 99.64 loss 0.07 | val acc 75.18 loss 0.76 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 73 | tr acc 99.76 loss 0.07 | val acc 75.48 loss 0.73 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 74 | tr acc 99.88 loss 0.07 | val acc 76.39 loss 0.72 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 75 | tr acc 99.97 loss 0.07 | val acc 75.70 loss 0.73 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 76 | tr acc 99.98 loss 0.06 | val acc 74.46 loss 0.76 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 77 | tr acc 93.37 loss 0.25 | val acc 68.11 loss 1.04 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 78 | tr acc 95.50 loss 0.18 | val acc 72.67 loss 0.84 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 79 | tr acc 99.25 loss 0.08 | val acc 74.96 loss 0.76 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 80 | tr acc 99.92 loss 0.06 | val acc 73.91 loss 0.79 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 81 | tr acc 100.00 loss 0.05 | val acc 74.82 loss 0.74 | best val acc 76.57 | best te acc 75.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82 | tr acc 100.00 loss 0.06 | val acc 75.84 loss 0.71 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 83 | tr acc 100.00 loss 0.06 | val acc 76.00 loss 0.70 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 84 | tr acc 100.00 loss 0.06 | val acc 76.05 loss 0.70 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 85 | tr acc 100.00 loss 0.06 | val acc 75.96 loss 0.71 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 86 | tr acc 95.97 loss 0.18 | val acc 57.98 loss 1.45 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 87 | tr acc 91.51 loss 0.30 | val acc 72.27 loss 0.87 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 88 | tr acc 98.41 loss 0.11 | val acc 73.98 loss 0.80 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 89 | tr acc 99.47 loss 0.07 | val acc 74.16 loss 0.78 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 90 | tr acc 99.81 loss 0.06 | val acc 74.94 loss 0.75 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 91 | tr acc 99.94 loss 0.06 | val acc 74.96 loss 0.73 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 92 | tr acc 100.00 loss 0.05 | val acc 74.98 loss 0.72 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 93 | tr acc 100.00 loss 0.06 | val acc 75.54 loss 0.71 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 94 | tr acc 100.00 loss 0.06 | val acc 75.76 loss 0.71 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 95 | tr acc 100.00 loss 0.06 | val acc 75.42 loss 0.72 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 96 | tr acc 100.00 loss 0.06 | val acc 75.16 loss 0.73 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 97 | tr acc 89.52 loss 0.36 | val acc 68.23 loss 1.04 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 98 | tr acc 96.91 loss 0.15 | val acc 73.01 loss 0.84 | best val acc 76.57 | best te acc 75.64\n",
      "epoch 99 | tr acc 98.77 loss 0.09 | val acc 73.03 loss 0.83 | best val acc 76.57 | best te acc 75.64\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir='./dmp'\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "batch_size=128\n",
    "gamma=0.5\n",
    "num_epochs=100\n",
    "\n",
    "model=PurchaseClassifier()\n",
    "model=model.cuda()\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=user_lr, weight_decay=1e-3)\n",
    "\n",
    "best_val_acc=0\n",
    "best_test_acc=0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     if epoch in schedule:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] *= gamma\n",
    "#             print('Epoch %d Local lr %f'%(epoch,param_group['lr']))\n",
    "\n",
    "    train_loss, train_acc = train(private_data_tensor, private_label_tensor, model, criterion, optimizer, epoch, use_cuda, uniform_reg=False)\n",
    "\n",
    "    val_loss, val_acc = test(val_data_tensor, val_label_tensor, model, criterion, use_cuda)\n",
    "\n",
    "    is_best = val_acc >= best_val_acc\n",
    "\n",
    "    best_val_acc=max(val_acc, best_val_acc)\n",
    "\n",
    "    if is_best:\n",
    "        _, best_test_acc = test(te_data_tensor,te_label_tensor,model,criterion,use_cuda)\n",
    "\n",
    "    save_checkpoint_global(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc': best_val_acc,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        },\n",
    "        is_best,\n",
    "        checkpoint=checkpoint_dir,\n",
    "        filename='unprotected_model.pth.tar',\n",
    "        best_filename='unprotected_model_best.pth.tar',\n",
    "    )\n",
    "\n",
    "    print('epoch %d | tr acc %.2f loss %.2f | val acc %.2f loss %.2f | best val acc %.2f | best te acc %.2f'\n",
    "          %(epoch, train_acc, train_loss, val_acc, val_loss, best_val_acc, best_test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unprotected model: train acc 99.4692 test acc 75.6431\n",
      "epoch 0 | distil loss 0.0330 | tr loss 2.6630 tr acc 23.0669 | val loss 2.6896 val acc 23.0105 | best val acc 23.0105 | best test acc 23.4124\n",
      "epoch 1 | distil loss 0.0137 | tr loss 1.3279 tr acc 57.1414 | val loss 1.4066 val acc 54.5418 | best val acc 54.5418 | best test acc 53.3963\n",
      "epoch 2 | distil loss 0.0078 | tr loss 1.2328 tr acc 58.2632 | val loss 1.3431 val acc 54.3810 | best val acc 54.5418 | best test acc 53.3963\n",
      "epoch 3 | distil loss 0.0061 | tr loss 1.1856 tr acc 59.2849 | val loss 1.3257 val acc 54.9839 | best val acc 54.9839 | best test acc 54.4815\n",
      "epoch 4 | distil loss 0.0055 | tr loss 1.1137 tr acc 62.0493 | val loss 1.2460 val acc 57.8376 | best val acc 57.8376 | best test acc 56.8931\n",
      "epoch 5 | distil loss 0.0053 | tr loss 1.0195 tr acc 65.3846 | val loss 1.1799 val acc 58.8023 | best val acc 58.8023 | best test acc 58.8023\n",
      "epoch 6 | distil loss 0.0044 | tr loss 0.9677 tr acc 67.3778 | val loss 1.1326 val acc 61.4349 | best val acc 61.4349 | best test acc 60.7918\n",
      "epoch 7 | distil loss 0.0043 | tr loss 0.9542 tr acc 67.5180 | val loss 1.1164 val acc 61.7765 | best val acc 61.7765 | best test acc 61.7363\n",
      "epoch 8 | distil loss 0.0045 | tr loss 0.9964 tr acc 66.1558 | val loss 1.1730 val acc 60.8320 | best val acc 61.7765 | best test acc 61.7363\n",
      "epoch 9 | distil loss 0.0042 | tr loss 0.9703 tr acc 66.9972 | val loss 1.1347 val acc 61.6359 | best val acc 61.7765 | best test acc 61.7363\n",
      "epoch 10 | distil loss 0.0036 | tr loss 0.8095 tr acc 73.7881 | val loss 0.9998 val acc 65.5145 | best val acc 65.5145 | best test acc 65.1929\n",
      "epoch 11 | distil loss 0.0033 | tr loss 0.7936 tr acc 73.7580 | val loss 0.9842 val acc 65.9767 | best val acc 65.9767 | best test acc 65.1929\n",
      "epoch 12 | distil loss 0.0027 | tr loss 0.7986 tr acc 73.5176 | val loss 1.0009 val acc 65.4542 | best val acc 65.9767 | best test acc 65.1929\n",
      "epoch 13 | distil loss 0.0025 | tr loss 0.7628 tr acc 74.5092 | val loss 0.9613 val acc 66.3384 | best val acc 66.3384 | best test acc 65.5748\n",
      "epoch 14 | distil loss 0.0023 | tr loss 0.8063 tr acc 73.0268 | val loss 1.0194 val acc 64.9518 | best val acc 66.3384 | best test acc 65.5748\n",
      "epoch 15 | distil loss 0.0027 | tr loss 0.7065 tr acc 76.9832 | val loss 0.9233 val acc 68.3883 | best val acc 68.3883 | best test acc 68.0064\n",
      "epoch 16 | distil loss 0.0027 | tr loss 0.7515 tr acc 75.0300 | val loss 0.9712 val acc 67.5442 | best val acc 68.3883 | best test acc 68.0064\n",
      "epoch 17 | distil loss 0.0023 | tr loss 0.6735 tr acc 77.7444 | val loss 0.9068 val acc 68.7299 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 18 | distil loss 0.0020 | tr loss 0.7272 tr acc 75.7312 | val loss 0.9619 val acc 66.5595 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 19 | distil loss 0.0025 | tr loss 0.7623 tr acc 75.1202 | val loss 0.9853 val acc 66.2982 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 20 | distil loss 0.0025 | tr loss 0.7294 tr acc 76.1118 | val loss 0.9718 val acc 66.7805 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 21 | distil loss 0.0024 | tr loss 0.7330 tr acc 76.2520 | val loss 0.9678 val acc 67.0217 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 22 | distil loss 0.0022 | tr loss 0.6866 tr acc 77.6242 | val loss 0.9165 val acc 68.5289 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 23 | distil loss 0.0024 | tr loss 0.7997 tr acc 73.0469 | val loss 1.0142 val acc 66.0571 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 24 | distil loss 0.0025 | tr loss 0.7998 tr acc 73.8381 | val loss 1.0301 val acc 65.3939 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 25 | distil loss 0.0024 | tr loss 0.6937 tr acc 77.8245 | val loss 0.9384 val acc 67.6648 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 26 | distil loss 0.0023 | tr loss 0.6878 tr acc 77.4339 | val loss 0.9346 val acc 67.5040 | best val acc 68.7299 | best test acc 68.4887\n",
      "epoch 27 | distil loss 0.0023 | tr loss 0.6552 tr acc 79.3369 | val loss 0.8930 val acc 69.1921 | best val acc 69.1921 | best test acc 69.8754\n",
      "epoch 28 | distil loss 0.0023 | tr loss 0.6936 tr acc 77.4740 | val loss 0.9281 val acc 68.1471 | best val acc 69.1921 | best test acc 69.8754\n",
      "epoch 29 | distil loss 0.0024 | tr loss 0.7298 tr acc 76.5024 | val loss 0.9640 val acc 67.5241 | best val acc 69.1921 | best test acc 69.8754\n",
      "epoch 30 | distil loss 0.0023 | tr loss 0.6618 tr acc 79.0164 | val loss 0.8867 val acc 69.7749 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 31 | distil loss 0.0023 | tr loss 0.7477 tr acc 75.2204 | val loss 0.9866 val acc 66.8006 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 32 | distil loss 0.0024 | tr loss 0.7186 tr acc 76.5325 | val loss 0.9835 val acc 66.3384 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 33 | distil loss 0.0021 | tr loss 0.6829 tr acc 77.5841 | val loss 0.9339 val acc 67.8457 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 34 | distil loss 0.0020 | tr loss 0.7031 tr acc 77.1234 | val loss 0.9601 val acc 68.2275 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 35 | distil loss 0.0021 | tr loss 0.8198 tr acc 72.4760 | val loss 1.0803 val acc 64.4293 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 36 | distil loss 0.0020 | tr loss 0.7138 tr acc 76.7728 | val loss 0.9943 val acc 66.4590 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 37 | distil loss 0.0020 | tr loss 0.6505 tr acc 78.8161 | val loss 0.9115 val acc 68.9108 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 38 | distil loss 0.0023 | tr loss 0.6888 tr acc 77.7544 | val loss 0.9342 val acc 68.0466 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 39 | distil loss 0.0023 | tr loss 0.6749 tr acc 78.3654 | val loss 0.9199 val acc 68.7701 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 40 | distil loss 0.0023 | tr loss 0.6908 tr acc 77.7744 | val loss 0.9538 val acc 67.4638 | best val acc 69.7749 | best test acc 69.3328\n",
      "epoch 41 | distil loss 0.0024 | tr loss 0.6495 tr acc 79.5373 | val loss 0.8958 val acc 69.7950 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 42 | distil loss 0.0024 | tr loss 0.8284 tr acc 73.0168 | val loss 1.0541 val acc 64.1680 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 43 | distil loss 0.0026 | tr loss 0.6735 tr acc 78.4155 | val loss 0.9148 val acc 68.3682 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 44 | distil loss 0.0019 | tr loss 0.6607 tr acc 78.6058 | val loss 0.9172 val acc 69.2725 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 45 | distil loss 0.0017 | tr loss 0.6705 tr acc 77.6242 | val loss 0.9305 val acc 68.2677 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 46 | distil loss 0.0017 | tr loss 0.6585 tr acc 77.9748 | val loss 0.9200 val acc 68.5691 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 47 | distil loss 0.0020 | tr loss 0.6807 tr acc 77.6542 | val loss 0.9373 val acc 67.4236 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 48 | distil loss 0.0022 | tr loss 0.7327 tr acc 75.7312 | val loss 0.9819 val acc 66.7605 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 49 | distil loss 0.0023 | tr loss 0.6957 tr acc 77.4038 | val loss 0.9456 val acc 67.5643 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 50 | distil loss 0.0025 | tr loss 0.6756 tr acc 78.5357 | val loss 0.9356 val acc 67.6447 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 51 | distil loss 0.0019 | tr loss 0.6114 tr acc 80.6390 | val loss 0.8781 val acc 69.2323 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 52 | distil loss 0.0022 | tr loss 0.6681 tr acc 79.0565 | val loss 0.9184 val acc 68.2074 | best val acc 69.7950 | best test acc 69.7347\n",
      "epoch 53 | distil loss 0.0023 | tr loss 0.6077 tr acc 81.3201 | val loss 0.8509 val acc 70.3175 | best val acc 70.3175 | best test acc 70.4180\n",
      "epoch 54 | distil loss 0.0022 | tr loss 0.6432 tr acc 79.8377 | val loss 0.8818 val acc 69.4132 | best val acc 70.3175 | best test acc 70.4180\n",
      "epoch 55 | distil loss 0.0020 | tr loss 0.6362 tr acc 79.9479 | val loss 0.8873 val acc 69.4132 | best val acc 70.3175 | best test acc 70.4180\n",
      "epoch 56 | distil loss 0.0015 | tr loss 0.5884 tr acc 82.0613 | val loss 0.8550 val acc 70.5788 | best val acc 70.5788 | best test acc 71.5233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 | distil loss 0.0013 | tr loss 0.5861 tr acc 82.0012 | val loss 0.8575 val acc 70.4180 | best val acc 70.5788 | best test acc 71.5233\n",
      "epoch 58 | distil loss 0.0014 | tr loss 0.6256 tr acc 80.0881 | val loss 0.8925 val acc 68.7902 | best val acc 70.5788 | best test acc 71.5233\n",
      "epoch 59 | distil loss 0.0016 | tr loss 0.6249 tr acc 80.5789 | val loss 0.8846 val acc 69.2122 | best val acc 70.5788 | best test acc 71.5233\n",
      "----> Epoch 60 distillation lr 0.010000\n",
      "epoch 60 | distil loss 0.0010 | tr loss 0.5090 tr acc 85.3365 | val loss 0.7792 val acc 73.5330 | best val acc 73.5330 | best test acc 73.1310\n",
      "epoch 61 | distil loss 0.0008 | tr loss 0.5016 tr acc 85.5869 | val loss 0.7726 val acc 73.7339 | best val acc 73.7339 | best test acc 73.3923\n",
      "epoch 62 | distil loss 0.0007 | tr loss 0.4974 tr acc 85.8273 | val loss 0.7689 val acc 73.8545 | best val acc 73.8545 | best test acc 73.3923\n",
      "epoch 63 | distil loss 0.0007 | tr loss 0.4941 tr acc 85.9575 | val loss 0.7662 val acc 73.9349 | best val acc 73.9349 | best test acc 73.5531\n",
      "epoch 64 | distil loss 0.0007 | tr loss 0.4915 tr acc 86.1178 | val loss 0.7642 val acc 73.8947 | best val acc 73.9349 | best test acc 73.5531\n",
      "epoch 65 | distil loss 0.0006 | tr loss 0.4892 tr acc 86.1979 | val loss 0.7626 val acc 73.9550 | best val acc 73.9550 | best test acc 73.5732\n",
      "epoch 66 | distil loss 0.0006 | tr loss 0.4872 tr acc 86.2380 | val loss 0.7613 val acc 74.0153 | best val acc 74.0153 | best test acc 73.6736\n",
      "epoch 67 | distil loss 0.0006 | tr loss 0.4855 tr acc 86.3582 | val loss 0.7601 val acc 74.0555 | best val acc 74.0555 | best test acc 73.6937\n",
      "epoch 68 | distil loss 0.0006 | tr loss 0.4838 tr acc 86.4884 | val loss 0.7592 val acc 74.0957 | best val acc 74.0957 | best test acc 73.7339\n",
      "epoch 69 | distil loss 0.0006 | tr loss 0.4824 tr acc 86.5385 | val loss 0.7584 val acc 74.0354 | best val acc 74.0957 | best test acc 73.7339\n",
      "epoch 70 | distil loss 0.0006 | tr loss 0.4810 tr acc 86.5785 | val loss 0.7577 val acc 74.0756 | best val acc 74.0957 | best test acc 73.7339\n",
      "epoch 71 | distil loss 0.0006 | tr loss 0.4797 tr acc 86.6286 | val loss 0.7570 val acc 74.1760 | best val acc 74.1760 | best test acc 73.6133\n",
      "epoch 72 | distil loss 0.0006 | tr loss 0.4785 tr acc 86.5785 | val loss 0.7565 val acc 74.2765 | best val acc 74.2765 | best test acc 73.6535\n",
      "epoch 73 | distil loss 0.0005 | tr loss 0.4774 tr acc 86.5885 | val loss 0.7560 val acc 74.1559 | best val acc 74.2765 | best test acc 73.6535\n",
      "epoch 74 | distil loss 0.0005 | tr loss 0.4764 tr acc 86.6286 | val loss 0.7556 val acc 74.1559 | best val acc 74.2765 | best test acc 73.6535\n",
      "epoch 75 | distil loss 0.0005 | tr loss 0.4754 tr acc 86.6987 | val loss 0.7552 val acc 74.1359 | best val acc 74.2765 | best test acc 73.6535\n",
      "epoch 76 | distil loss 0.0005 | tr loss 0.4745 tr acc 86.6587 | val loss 0.7548 val acc 74.0957 | best val acc 74.2765 | best test acc 73.6535\n",
      "epoch 77 | distil loss 0.0005 | tr loss 0.4736 tr acc 86.6587 | val loss 0.7545 val acc 74.0957 | best val acc 74.2765 | best test acc 73.6535\n",
      "epoch 78 | distil loss 0.0005 | tr loss 0.4728 tr acc 86.7288 | val loss 0.7542 val acc 74.1559 | best val acc 74.2765 | best test acc 73.6535\n",
      "epoch 79 | distil loss 0.0005 | tr loss 0.4720 tr acc 86.7889 | val loss 0.7539 val acc 74.2162 | best val acc 74.2765 | best test acc 73.6535\n",
      "epoch 80 | distil loss 0.0005 | tr loss 0.4712 tr acc 86.7488 | val loss 0.7536 val acc 74.2966 | best val acc 74.2966 | best test acc 73.6736\n",
      "epoch 81 | distil loss 0.0005 | tr loss 0.4705 tr acc 86.8089 | val loss 0.7534 val acc 74.3770 | best val acc 74.3770 | best test acc 73.7339\n",
      "epoch 82 | distil loss 0.0005 | tr loss 0.4698 tr acc 86.8389 | val loss 0.7531 val acc 74.4373 | best val acc 74.4373 | best test acc 73.7540\n",
      "epoch 83 | distil loss 0.0005 | tr loss 0.4691 tr acc 86.8690 | val loss 0.7529 val acc 74.4172 | best val acc 74.4373 | best test acc 73.7540\n",
      "epoch 84 | distil loss 0.0005 | tr loss 0.4684 tr acc 86.8490 | val loss 0.7527 val acc 74.4574 | best val acc 74.4574 | best test acc 73.7339\n",
      "epoch 85 | distil loss 0.0005 | tr loss 0.4678 tr acc 86.8690 | val loss 0.7525 val acc 74.4775 | best val acc 74.4775 | best test acc 73.7138\n",
      "epoch 86 | distil loss 0.0005 | tr loss 0.4672 tr acc 86.9491 | val loss 0.7523 val acc 74.5177 | best val acc 74.5177 | best test acc 73.6736\n",
      "epoch 87 | distil loss 0.0005 | tr loss 0.4666 tr acc 87.0092 | val loss 0.7521 val acc 74.5177 | best val acc 74.5177 | best test acc 73.6937\n",
      "epoch 88 | distil loss 0.0005 | tr loss 0.4660 tr acc 87.0393 | val loss 0.7519 val acc 74.5780 | best val acc 74.5780 | best test acc 73.6133\n",
      "epoch 89 | distil loss 0.0005 | tr loss 0.4654 tr acc 87.0893 | val loss 0.7517 val acc 74.6182 | best val acc 74.6182 | best test acc 73.6535\n",
      "----> Epoch 90 distillation lr 0.001000\n",
      "epoch 90 | distil loss 0.0005 | tr loss 0.4618 tr acc 87.4599 | val loss 0.7480 val acc 74.6182 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 91 | distil loss 0.0005 | tr loss 0.4621 tr acc 87.4199 | val loss 0.7485 val acc 74.5579 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 92 | distil loss 0.0005 | tr loss 0.4622 tr acc 87.4099 | val loss 0.7486 val acc 74.5579 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 93 | distil loss 0.0005 | tr loss 0.4622 tr acc 87.4199 | val loss 0.7485 val acc 74.5579 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 94 | distil loss 0.0005 | tr loss 0.4621 tr acc 87.4299 | val loss 0.7485 val acc 74.5579 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 95 | distil loss 0.0005 | tr loss 0.4621 tr acc 87.4399 | val loss 0.7484 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 96 | distil loss 0.0005 | tr loss 0.4620 tr acc 87.4399 | val loss 0.7484 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 97 | distil loss 0.0005 | tr loss 0.4620 tr acc 87.4399 | val loss 0.7483 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 98 | distil loss 0.0005 | tr loss 0.4619 tr acc 87.4299 | val loss 0.7483 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 99 | distil loss 0.0005 | tr loss 0.4619 tr acc 87.4399 | val loss 0.7483 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 100 | distil loss 0.0005 | tr loss 0.4618 tr acc 87.4499 | val loss 0.7482 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 101 | distil loss 0.0005 | tr loss 0.4618 tr acc 87.4399 | val loss 0.7482 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 102 | distil loss 0.0005 | tr loss 0.4617 tr acc 87.4599 | val loss 0.7482 val acc 74.4775 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 103 | distil loss 0.0005 | tr loss 0.4617 tr acc 87.4599 | val loss 0.7481 val acc 74.4775 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 104 | distil loss 0.0005 | tr loss 0.4616 tr acc 87.4700 | val loss 0.7481 val acc 74.4775 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 105 | distil loss 0.0005 | tr loss 0.4616 tr acc 87.4800 | val loss 0.7481 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 106 | distil loss 0.0005 | tr loss 0.4615 tr acc 87.4800 | val loss 0.7480 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 107 | distil loss 0.0005 | tr loss 0.4615 tr acc 87.4900 | val loss 0.7480 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 108 | distil loss 0.0005 | tr loss 0.4615 tr acc 87.4900 | val loss 0.7480 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 109 | distil loss 0.0005 | tr loss 0.4614 tr acc 87.4900 | val loss 0.7480 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 110 | distil loss 0.0005 | tr loss 0.4614 tr acc 87.4900 | val loss 0.7479 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 111 | distil loss 0.0005 | tr loss 0.4613 tr acc 87.4800 | val loss 0.7479 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 112 | distil loss 0.0005 | tr loss 0.4613 tr acc 87.4900 | val loss 0.7479 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 113 | distil loss 0.0005 | tr loss 0.4612 tr acc 87.5000 | val loss 0.7478 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 114 | distil loss 0.0005 | tr loss 0.4612 tr acc 87.5000 | val loss 0.7478 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 115 | distil loss 0.0005 | tr loss 0.4611 tr acc 87.5000 | val loss 0.7478 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 116 | distil loss 0.0005 | tr loss 0.4611 tr acc 87.5100 | val loss 0.7478 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 117 | distil loss 0.0005 | tr loss 0.4610 tr acc 87.5200 | val loss 0.7477 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 118 | distil loss 0.0005 | tr loss 0.4610 tr acc 87.5200 | val loss 0.7477 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 119 | distil loss 0.0005 | tr loss 0.4609 tr acc 87.5200 | val loss 0.7477 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 120 | distil loss 0.0005 | tr loss 0.4609 tr acc 87.5100 | val loss 0.7477 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 121 | distil loss 0.0005 | tr loss 0.4608 tr acc 87.5100 | val loss 0.7476 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 122 | distil loss 0.0005 | tr loss 0.4608 tr acc 87.5100 | val loss 0.7476 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 123 | distil loss 0.0005 | tr loss 0.4607 tr acc 87.5200 | val loss 0.7476 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 124 | distil loss 0.0005 | tr loss 0.4607 tr acc 87.5200 | val loss 0.7476 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 125 | distil loss 0.0005 | tr loss 0.4606 tr acc 87.5200 | val loss 0.7475 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 126 | distil loss 0.0005 | tr loss 0.4606 tr acc 87.5300 | val loss 0.7475 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 127 | distil loss 0.0005 | tr loss 0.4605 tr acc 87.5501 | val loss 0.7475 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 128 | distil loss 0.0005 | tr loss 0.4605 tr acc 87.5401 | val loss 0.7475 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 129 | distil loss 0.0005 | tr loss 0.4605 tr acc 87.5501 | val loss 0.7475 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 130 | distil loss 0.0005 | tr loss 0.4604 tr acc 87.5501 | val loss 0.7474 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 131 | distil loss 0.0005 | tr loss 0.4604 tr acc 87.5601 | val loss 0.7474 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 132 | distil loss 0.0005 | tr loss 0.4603 tr acc 87.5601 | val loss 0.7474 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 133 | distil loss 0.0005 | tr loss 0.4603 tr acc 87.5601 | val loss 0.7474 val acc 74.5378 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 134 | distil loss 0.0005 | tr loss 0.4602 tr acc 87.5601 | val loss 0.7473 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 135 | distil loss 0.0005 | tr loss 0.4602 tr acc 87.5601 | val loss 0.7473 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 136 | distil loss 0.0005 | tr loss 0.4601 tr acc 87.5801 | val loss 0.7473 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 137 | distil loss 0.0005 | tr loss 0.4601 tr acc 87.5801 | val loss 0.7473 val acc 74.4775 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 138 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.5701 | val loss 0.7472 val acc 74.4775 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 139 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.5801 | val loss 0.7472 val acc 74.4775 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 140 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.5801 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 141 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5901 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 142 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.6002 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 143 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.6002 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 144 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.6002 | val loss 0.7471 val acc 74.4775 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 145 | distil loss 0.0005 | tr loss 0.4597 tr acc 87.6002 | val loss 0.7471 val acc 74.4574 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 146 | distil loss 0.0005 | tr loss 0.4597 tr acc 87.6102 | val loss 0.7471 val acc 74.4172 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 147 | distil loss 0.0005 | tr loss 0.4596 tr acc 87.6102 | val loss 0.7470 val acc 74.4172 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 148 | distil loss 0.0005 | tr loss 0.4596 tr acc 87.6102 | val loss 0.7470 val acc 74.3971 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 149 | distil loss 0.0005 | tr loss 0.4595 tr acc 87.6102 | val loss 0.7470 val acc 74.3971 | best val acc 74.6182 | best test acc 73.9550\n",
      "----> Epoch 150 distillation lr 0.000100\n",
      "epoch 150 | distil loss 0.0005 | tr loss 0.4597 tr acc 87.5000 | val loss 0.7471 val acc 74.4172 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 151 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5300 | val loss 0.7471 val acc 74.4574 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 152 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.5000 | val loss 0.7472 val acc 74.4775 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 153 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4900 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 154 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4599 | val loss 0.7473 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 155 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4499 | val loss 0.7473 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 156 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4499 | val loss 0.7473 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 157 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4499 | val loss 0.7473 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 158 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4499 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 159 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4499 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 160 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4599 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 161 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4599 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 162 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4599 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 163 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4700 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 164 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4700 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 165 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4700 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 166 | distil loss 0.0005 | tr loss 0.4600 tr acc 87.4700 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 167 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.4800 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 168 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.4800 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 169 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.4900 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 170 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.4900 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 171 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.4900 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 172 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.4900 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 173 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 174 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 175 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 176 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 177 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 178 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 179 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 180 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 181 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.5177 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 182 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 183 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5100 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 184 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5200 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 185 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5200 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 186 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5200 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 187 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5200 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 188 | distil loss 0.0005 | tr loss 0.4599 tr acc 87.5200 | val loss 0.7472 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 189 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5200 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 190 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 191 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 192 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 193 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 194 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 195 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 196 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 197 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 198 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n",
      "epoch 199 | distil loss 0.0005 | tr loss 0.4598 tr acc 87.5300 | val loss 0.7471 val acc 74.4976 | best val acc 74.6182 | best test acc 73.9550\n"
     ]
    }
   ],
   "source": [
    "# distil the knowledge of the unprotected model in the ref data\n",
    "\n",
    "best_model=PurchaseClassifier().cuda()\n",
    "\n",
    "resume_best=checkpoint_dir+'/unprotected_model_best.pth.tar'\n",
    "\n",
    "assert os.path.isfile(resume_best), 'Error: no checkpoint directory found for best model'\n",
    "checkpoint = os.path.dirname(resume_best)\n",
    "checkpoint = torch.load(resume_best)\n",
    "best_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "_,best_test = test(te_data_tensor, te_label_tensor, best_model, criterion, use_cuda)\n",
    "_,best_train = test(private_data_tensor, private_label_tensor, best_model, criterion, use_cuda)\n",
    "print('unprotected model: train acc %.4f test acc %.4f'%(best_train, best_test))\n",
    "\n",
    "batch_size=100\n",
    "all_outputs=[]\n",
    "\n",
    "len_t = len(ref_data_tensor)//batch_size\n",
    "\n",
    "for ind in range(len_t):\n",
    "    inputs = ref_data_tensor[ind*batch_size:(ind+1)*batch_size]\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    inputs = torch.autograd.Variable(inputs)\n",
    "    outputs,_,_ = best_model(inputs)\n",
    "    all_outputs.append(outputs.data.cpu().numpy())\n",
    "\n",
    "if len(ref_data_tensor)%batch_size:\n",
    "    inputs=ref_data_tensor[-(len(target_ref_data_tensor)%batch_size):]\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    inputs = torch.autograd.Variable(inputs)\n",
    "    outputs,_,_ = best_model(inputs)\n",
    "    all_outputs.append(outputs.data.cpu().numpy())\n",
    "\n",
    "final_outputs=np.concatenate(all_outputs)\n",
    "distil_label_tensor=(torch.from_numpy(final_outputs).type(torch.FloatTensor))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train final protected model via knowledge distillation\n",
    "\n",
    "distil_model=PurchaseClassifier().cuda()\n",
    "distil_test_criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "distil_schedule=[60, 90, 150]\n",
    "distil_lr=.1\n",
    "distil_epochs=200\n",
    "\n",
    "distil_best_acc=0\n",
    "best_distil_test_acc=0\n",
    "gamma=.1\n",
    "t_softmax=1\n",
    "\n",
    "for epoch in range(distil_epochs):\n",
    "    if epoch in distil_schedule:\n",
    "        distil_lr *= gamma\n",
    "        print('----> Epoch %d distillation lr %f'%(epoch,distil_lr))\n",
    "\n",
    "    distil_optimizer=optim.SGD(distil_model.parameters(), lr=distil_lr, momentum=0.99, weight_decay=1e-5)\n",
    "\n",
    "    distil_tr_loss = train_pub(ref_data_tensor, distil_label_tensor, ref_label_tensor, distil_model, t_softmax,\n",
    "                               distil_optimizer, batch_size=16, alpha=1)\n",
    "\n",
    "    tr_loss,tr_acc = test(private_data_tensor, private_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "    \n",
    "    val_loss,val_acc = test(val_data_tensor, val_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "\n",
    "    distil_is_best = val_acc >= distil_best_acc\n",
    "\n",
    "    distil_best_acc=max(val_acc, distil_best_acc)\n",
    "\n",
    "    if distil_is_best:\n",
    "        _,best_distil_test_acc = test(te_data_tensor, te_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "\n",
    "    save_checkpoint_global(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': distil_model.state_dict(),\n",
    "            'best_acc': distil_best_acc,\n",
    "            'optimizer': distil_optimizer.state_dict(),\n",
    "        },\n",
    "        distil_is_best,\n",
    "        checkpoint=checkpoint_dir,\n",
    "        filename='protected_model.pth.tar',\n",
    "        best_filename='protected_model_best.pth.tar',\n",
    "    )\n",
    "\n",
    "    print('epoch %d | distil loss %.4f | tr loss %.4f tr acc %.4f | val loss %.4f val acc %.4f | best val acc %.4f | best test acc %.4f'%(epoch,distil_tr_loss,tr_loss,tr_acc,val_loss,val_acc,distil_best_acc,best_distil_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | distil loss 0.0383 | tr loss 4.2935 tr acc 3.5757 | val loss 4.3025 val acc 3.2958 | best val acc 3.2958 | best test acc 3.0949\n",
      "epoch 1 | distil loss 0.0289 | tr loss 2.6946 tr acc 24.7596 | val loss 2.7269 val acc 23.8947 | best val acc 23.8947 | best test acc 23.6937\n",
      "epoch 2 | distil loss 0.0171 | tr loss 1.8594 tr acc 44.6915 | val loss 1.9134 val acc 42.8457 | best val acc 42.8457 | best test acc 42.5844\n",
      "epoch 3 | distil loss 0.0103 | tr loss 1.4389 tr acc 55.1082 | val loss 1.5171 val acc 52.1302 | best val acc 52.1302 | best test acc 51.9494\n",
      "epoch 4 | distil loss 0.0075 | tr loss 1.1874 tr acc 62.7304 | val loss 1.2945 val acc 58.1793 | best val acc 58.1793 | best test acc 57.6166\n",
      "epoch 5 | distil loss 0.0063 | tr loss 1.1429 tr acc 62.6903 | val loss 1.2655 val acc 58.2998 | best val acc 58.2998 | best test acc 57.2749\n",
      "epoch 6 | distil loss 0.0057 | tr loss 1.0464 tr acc 66.0056 | val loss 1.1792 val acc 60.9325 | best val acc 60.9325 | best test acc 60.1889\n",
      "epoch 7 | distil loss 0.0050 | tr loss 1.0627 tr acc 64.1727 | val loss 1.2089 val acc 58.7018 | best val acc 60.9325 | best test acc 60.1889\n",
      "epoch 8 | distil loss 0.0046 | tr loss 0.9973 tr acc 66.0557 | val loss 1.1516 val acc 61.0732 | best val acc 61.0732 | best test acc 59.8875\n",
      "epoch 9 | distil loss 0.0045 | tr loss 0.9663 tr acc 66.9371 | val loss 1.1333 val acc 60.4904 | best val acc 61.0732 | best test acc 59.8875\n",
      "epoch 10 | distil loss 0.0044 | tr loss 0.9871 tr acc 66.1158 | val loss 1.1560 val acc 61.1133 | best val acc 61.1133 | best test acc 60.4301\n",
      "epoch 11 | distil loss 0.0041 | tr loss 0.9556 tr acc 67.3578 | val loss 1.1341 val acc 61.5354 | best val acc 61.5354 | best test acc 60.4301\n",
      "epoch 12 | distil loss 0.0043 | tr loss 0.9428 tr acc 67.9387 | val loss 1.1148 val acc 61.8971 | best val acc 61.8971 | best test acc 60.9928\n",
      "epoch 13 | distil loss 0.0042 | tr loss 0.8703 tr acc 71.2039 | val loss 1.0547 val acc 63.5249 | best val acc 63.5249 | best test acc 63.4043\n",
      "epoch 14 | distil loss 0.0037 | tr loss 0.8698 tr acc 70.9335 | val loss 1.0520 val acc 64.0474 | best val acc 64.0474 | best test acc 63.1230\n",
      "epoch 15 | distil loss 0.0033 | tr loss 0.9622 tr acc 67.2276 | val loss 1.1386 val acc 60.2894 | best val acc 64.0474 | best test acc 63.1230\n",
      "epoch 16 | distil loss 0.0035 | tr loss 0.8557 tr acc 71.3442 | val loss 1.0495 val acc 64.0273 | best val acc 64.0474 | best test acc 63.1230\n",
      "epoch 17 | distil loss 0.0034 | tr loss 0.9510 tr acc 67.9387 | val loss 1.1560 val acc 61.2540 | best val acc 64.0474 | best test acc 63.1230\n",
      "epoch 18 | distil loss 0.0034 | tr loss 0.8440 tr acc 72.0853 | val loss 1.0285 val acc 65.2934 | best val acc 65.2934 | best test acc 64.0273\n",
      "epoch 19 | distil loss 0.0027 | tr loss 0.7454 tr acc 75.8413 | val loss 0.9382 val acc 68.4887 | best val acc 68.4887 | best test acc 66.7805\n",
      "epoch 20 | distil loss 0.0024 | tr loss 0.7064 tr acc 77.2236 | val loss 0.9096 val acc 68.4686 | best val acc 68.4887 | best test acc 66.7805\n",
      "epoch 21 | distil loss 0.0023 | tr loss 0.8421 tr acc 71.2941 | val loss 1.0488 val acc 64.4895 | best val acc 68.4887 | best test acc 66.7805\n",
      "epoch 22 | distil loss 0.0022 | tr loss 0.7984 tr acc 73.3974 | val loss 1.0166 val acc 65.3336 | best val acc 68.4887 | best test acc 66.7805\n",
      "epoch 23 | distil loss 0.0021 | tr loss 0.7051 tr acc 77.4439 | val loss 0.9293 val acc 68.3280 | best val acc 68.4887 | best test acc 66.7805\n",
      "epoch 24 | distil loss 0.0021 | tr loss 0.7330 tr acc 76.0717 | val loss 0.9547 val acc 68.1873 | best val acc 68.4887 | best test acc 66.7805\n",
      "epoch 25 | distil loss 0.0021 | tr loss 0.6994 tr acc 77.4639 | val loss 0.9223 val acc 68.7902 | best val acc 68.7902 | best test acc 67.0217\n",
      "epoch 26 | distil loss 0.0023 | tr loss 0.7051 tr acc 77.3638 | val loss 0.9373 val acc 68.5088 | best val acc 68.7902 | best test acc 67.0217\n",
      "epoch 27 | distil loss 0.0021 | tr loss 0.7357 tr acc 75.4307 | val loss 0.9743 val acc 67.1624 | best val acc 68.7902 | best test acc 67.0217\n",
      "epoch 28 | distil loss 0.0019 | tr loss 0.7128 tr acc 76.5224 | val loss 0.9493 val acc 67.6648 | best val acc 68.7902 | best test acc 67.0217\n",
      "epoch 29 | distil loss 0.0018 | tr loss 0.6622 tr acc 78.7059 | val loss 0.9019 val acc 69.2725 | best val acc 69.2725 | best test acc 69.0916\n",
      "epoch 30 | distil loss 0.0019 | tr loss 0.6831 tr acc 77.6843 | val loss 0.9204 val acc 69.0113 | best val acc 69.2725 | best test acc 69.0916\n",
      "epoch 31 | distil loss 0.0020 | tr loss 0.7788 tr acc 73.8281 | val loss 1.0210 val acc 65.6150 | best val acc 69.2725 | best test acc 69.0916\n",
      "epoch 32 | distil loss 0.0020 | tr loss 0.6829 tr acc 77.8345 | val loss 0.9294 val acc 68.4486 | best val acc 69.2725 | best test acc 69.0916\n",
      "epoch 33 | distil loss 0.0021 | tr loss 0.7275 tr acc 76.3722 | val loss 0.9817 val acc 66.5394 | best val acc 69.2725 | best test acc 69.0916\n",
      "epoch 34 | distil loss 0.0021 | tr loss 0.8764 tr acc 70.5829 | val loss 1.1168 val acc 62.3794 | best val acc 69.2725 | best test acc 69.0916\n",
      "epoch 35 | distil loss 0.0022 | tr loss 0.6699 tr acc 78.2652 | val loss 0.9054 val acc 68.9510 | best val acc 69.2725 | best test acc 69.0916\n",
      "epoch 36 | distil loss 0.0021 | tr loss 0.6462 tr acc 79.1567 | val loss 0.8812 val acc 69.9558 | best val acc 69.9558 | best test acc 68.7902\n",
      "epoch 37 | distil loss 0.0021 | tr loss 0.7880 tr acc 73.6278 | val loss 1.0306 val acc 65.2532 | best val acc 69.9558 | best test acc 68.7902\n",
      "epoch 38 | distil loss 0.0017 | tr loss 0.6458 tr acc 79.6875 | val loss 0.9015 val acc 69.3127 | best val acc 69.9558 | best test acc 68.7902\n",
      "epoch 39 | distil loss 0.0015 | tr loss 0.6263 tr acc 79.8978 | val loss 0.8883 val acc 70.2773 | best val acc 70.2773 | best test acc 69.6141\n",
      "epoch 40 | distil loss 0.0014 | tr loss 0.6388 tr acc 79.3870 | val loss 0.9059 val acc 68.9510 | best val acc 70.2773 | best test acc 69.6141\n",
      "epoch 41 | distil loss 0.0015 | tr loss 0.6593 tr acc 78.7760 | val loss 0.9194 val acc 68.5088 | best val acc 70.2773 | best test acc 69.6141\n",
      "epoch 42 | distil loss 0.0016 | tr loss 0.6065 tr acc 81.4002 | val loss 0.8689 val acc 70.2170 | best val acc 70.2773 | best test acc 69.6141\n",
      "epoch 43 | distil loss 0.0014 | tr loss 0.6639 tr acc 78.3454 | val loss 0.9275 val acc 68.4285 | best val acc 70.2773 | best test acc 69.6141\n",
      "epoch 44 | distil loss 0.0014 | tr loss 0.5958 tr acc 81.5204 | val loss 0.8654 val acc 70.9003 | best val acc 70.9003 | best test acc 69.7749\n",
      "epoch 45 | distil loss 0.0015 | tr loss 0.6101 tr acc 80.8293 | val loss 0.8757 val acc 70.2572 | best val acc 70.9003 | best test acc 69.7749\n",
      "epoch 46 | distil loss 0.0017 | tr loss 0.6237 tr acc 79.9880 | val loss 0.8921 val acc 69.7749 | best val acc 70.9003 | best test acc 69.7749\n",
      "epoch 47 | distil loss 0.0017 | tr loss 0.7580 tr acc 74.7997 | val loss 1.0294 val acc 65.6953 | best val acc 70.9003 | best test acc 69.7749\n",
      "epoch 48 | distil loss 0.0016 | tr loss 0.6469 tr acc 79.3269 | val loss 0.9178 val acc 68.5088 | best val acc 70.9003 | best test acc 69.7749\n",
      "epoch 49 | distil loss 0.0016 | tr loss 0.6987 tr acc 77.7945 | val loss 0.9706 val acc 67.5442 | best val acc 70.9003 | best test acc 69.7749\n",
      "epoch 50 | distil loss 0.0015 | tr loss 0.6469 tr acc 79.8978 | val loss 0.9203 val acc 68.5088 | best val acc 70.9003 | best test acc 69.7749\n",
      "epoch 51 | distil loss 0.0014 | tr loss 0.5754 tr acc 82.4219 | val loss 0.8375 val acc 71.1616 | best val acc 71.1616 | best test acc 70.4180\n",
      "epoch 52 | distil loss 0.0016 | tr loss 0.6245 tr acc 80.3185 | val loss 0.8874 val acc 69.6744 | best val acc 71.1616 | best test acc 70.4180\n",
      "epoch 53 | distil loss 0.0018 | tr loss 0.7389 tr acc 76.1118 | val loss 1.0201 val acc 65.7757 | best val acc 71.1616 | best test acc 70.4180\n",
      "epoch 54 | distil loss 0.0016 | tr loss 0.5843 tr acc 81.9311 | val loss 0.8630 val acc 70.5989 | best val acc 71.1616 | best test acc 70.4180\n",
      "epoch 55 | distil loss 0.0017 | tr loss 0.6812 tr acc 78.3454 | val loss 0.9557 val acc 66.6198 | best val acc 71.1616 | best test acc 70.4180\n",
      "epoch 56 | distil loss 0.0020 | tr loss 0.8193 tr acc 73.0068 | val loss 1.0809 val acc 64.1278 | best val acc 71.1616 | best test acc 70.4180\n",
      "epoch 57 | distil loss 0.0023 | tr loss 0.8996 tr acc 70.1122 | val loss 1.1698 val acc 61.5555 | best val acc 71.1616 | best test acc 70.4180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 | distil loss 0.0021 | tr loss 0.6231 tr acc 80.5389 | val loss 0.8757 val acc 69.9960 | best val acc 71.1616 | best test acc 70.4180\n",
      "epoch 59 | distil loss 0.0016 | tr loss 0.6362 tr acc 80.0581 | val loss 0.9010 val acc 70.0563 | best val acc 71.1616 | best test acc 70.4180\n",
      "----> Epoch 60 distillation lr 0.010000\n",
      "epoch 60 | distil loss 0.0011 | tr loss 0.5249 tr acc 84.7456 | val loss 0.7910 val acc 73.5932 | best val acc 73.5932 | best test acc 72.5080\n",
      "epoch 61 | distil loss 0.0008 | tr loss 0.5218 tr acc 84.8958 | val loss 0.7888 val acc 73.4727 | best val acc 73.5932 | best test acc 72.5080\n",
      "epoch 62 | distil loss 0.0008 | tr loss 0.5197 tr acc 84.8558 | val loss 0.7869 val acc 73.4325 | best val acc 73.5932 | best test acc 72.5080\n",
      "epoch 63 | distil loss 0.0008 | tr loss 0.5179 tr acc 84.9259 | val loss 0.7853 val acc 73.5531 | best val acc 73.5932 | best test acc 72.5080\n",
      "epoch 64 | distil loss 0.0008 | tr loss 0.5163 tr acc 85.0260 | val loss 0.7840 val acc 73.5732 | best val acc 73.5932 | best test acc 72.5080\n",
      "epoch 65 | distil loss 0.0008 | tr loss 0.5149 tr acc 85.0661 | val loss 0.7829 val acc 73.4928 | best val acc 73.5932 | best test acc 72.5080\n",
      "epoch 66 | distil loss 0.0008 | tr loss 0.5135 tr acc 85.1963 | val loss 0.7819 val acc 73.6535 | best val acc 73.6535 | best test acc 72.8095\n",
      "epoch 67 | distil loss 0.0007 | tr loss 0.5123 tr acc 85.2764 | val loss 0.7811 val acc 73.7741 | best val acc 73.7741 | best test acc 72.8698\n",
      "epoch 68 | distil loss 0.0007 | tr loss 0.5112 tr acc 85.3566 | val loss 0.7803 val acc 73.7339 | best val acc 73.7741 | best test acc 72.8698\n",
      "epoch 69 | distil loss 0.0007 | tr loss 0.5101 tr acc 85.4067 | val loss 0.7797 val acc 73.7339 | best val acc 73.7741 | best test acc 72.8698\n",
      "epoch 70 | distil loss 0.0007 | tr loss 0.5091 tr acc 85.4367 | val loss 0.7790 val acc 73.7540 | best val acc 73.7741 | best test acc 72.8698\n",
      "epoch 71 | distil loss 0.0007 | tr loss 0.5081 tr acc 85.4567 | val loss 0.7785 val acc 73.7138 | best val acc 73.7741 | best test acc 72.8698\n",
      "epoch 72 | distil loss 0.0007 | tr loss 0.5072 tr acc 85.4868 | val loss 0.7780 val acc 73.7540 | best val acc 73.7741 | best test acc 72.8698\n",
      "epoch 73 | distil loss 0.0007 | tr loss 0.5063 tr acc 85.5068 | val loss 0.7775 val acc 73.7942 | best val acc 73.7942 | best test acc 73.0105\n",
      "epoch 74 | distil loss 0.0007 | tr loss 0.5055 tr acc 85.5869 | val loss 0.7771 val acc 73.9349 | best val acc 73.9349 | best test acc 72.9502\n",
      "epoch 75 | distil loss 0.0007 | tr loss 0.5047 tr acc 85.6370 | val loss 0.7767 val acc 73.9148 | best val acc 73.9349 | best test acc 72.9502\n",
      "epoch 76 | distil loss 0.0007 | tr loss 0.5040 tr acc 85.6771 | val loss 0.7764 val acc 73.8746 | best val acc 73.9349 | best test acc 72.9502\n",
      "epoch 77 | distil loss 0.0007 | tr loss 0.5033 tr acc 85.6971 | val loss 0.7761 val acc 73.7942 | best val acc 73.9349 | best test acc 72.9502\n",
      "epoch 78 | distil loss 0.0007 | tr loss 0.5026 tr acc 85.7272 | val loss 0.7758 val acc 73.8344 | best val acc 73.9349 | best test acc 72.9502\n",
      "epoch 79 | distil loss 0.0007 | tr loss 0.5019 tr acc 85.7472 | val loss 0.7755 val acc 73.8947 | best val acc 73.9349 | best test acc 72.9502\n",
      "epoch 80 | distil loss 0.0007 | tr loss 0.5013 tr acc 85.7672 | val loss 0.7753 val acc 73.9550 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 81 | distil loss 0.0007 | tr loss 0.5006 tr acc 85.7873 | val loss 0.7750 val acc 73.9349 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 82 | distil loss 0.0007 | tr loss 0.5000 tr acc 85.8073 | val loss 0.7748 val acc 73.9148 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 83 | distil loss 0.0007 | tr loss 0.4995 tr acc 85.8273 | val loss 0.7746 val acc 73.8947 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 84 | distil loss 0.0007 | tr loss 0.4989 tr acc 85.8674 | val loss 0.7744 val acc 73.8947 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 85 | distil loss 0.0006 | tr loss 0.4983 tr acc 85.8674 | val loss 0.7742 val acc 73.8143 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 86 | distil loss 0.0006 | tr loss 0.4978 tr acc 85.8974 | val loss 0.7741 val acc 73.7942 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 87 | distil loss 0.0006 | tr loss 0.4973 tr acc 85.9375 | val loss 0.7739 val acc 73.8143 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 88 | distil loss 0.0006 | tr loss 0.4968 tr acc 85.9375 | val loss 0.7738 val acc 73.7942 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 89 | distil loss 0.0006 | tr loss 0.4963 tr acc 85.9175 | val loss 0.7736 val acc 73.7138 | best val acc 73.9550 | best test acc 73.0105\n",
      "----> Epoch 90 distillation lr 0.001000\n",
      "epoch 90 | distil loss 0.0006 | tr loss 0.4930 tr acc 86.2480 | val loss 0.7702 val acc 73.9349 | best val acc 73.9550 | best test acc 73.0105\n",
      "epoch 91 | distil loss 0.0006 | tr loss 0.4929 tr acc 86.2680 | val loss 0.7703 val acc 74.0354 | best val acc 74.0354 | best test acc 73.2315\n",
      "epoch 92 | distil loss 0.0006 | tr loss 0.4928 tr acc 86.2680 | val loss 0.7702 val acc 74.0153 | best val acc 74.0354 | best test acc 73.2315\n",
      "epoch 93 | distil loss 0.0006 | tr loss 0.4928 tr acc 86.2680 | val loss 0.7702 val acc 74.0354 | best val acc 74.0354 | best test acc 73.2114\n",
      "epoch 94 | distil loss 0.0006 | tr loss 0.4928 tr acc 86.2881 | val loss 0.7702 val acc 74.0354 | best val acc 74.0354 | best test acc 73.2315\n",
      "epoch 95 | distil loss 0.0006 | tr loss 0.4927 tr acc 86.3081 | val loss 0.7702 val acc 74.0354 | best val acc 74.0354 | best test acc 73.2114\n",
      "epoch 96 | distil loss 0.0006 | tr loss 0.4927 tr acc 86.3181 | val loss 0.7702 val acc 74.0153 | best val acc 74.0354 | best test acc 73.2114\n",
      "epoch 97 | distil loss 0.0006 | tr loss 0.4926 tr acc 86.3081 | val loss 0.7702 val acc 74.0153 | best val acc 74.0354 | best test acc 73.2114\n",
      "epoch 98 | distil loss 0.0006 | tr loss 0.4926 tr acc 86.3281 | val loss 0.7702 val acc 74.0153 | best val acc 74.0354 | best test acc 73.2114\n",
      "epoch 99 | distil loss 0.0006 | tr loss 0.4926 tr acc 86.3081 | val loss 0.7702 val acc 74.0153 | best val acc 74.0354 | best test acc 73.2114\n",
      "epoch 100 | distil loss 0.0006 | tr loss 0.4925 tr acc 86.3081 | val loss 0.7702 val acc 74.0354 | best val acc 74.0354 | best test acc 73.2315\n",
      "epoch 101 | distil loss 0.0006 | tr loss 0.4925 tr acc 86.3081 | val loss 0.7702 val acc 74.0555 | best val acc 74.0555 | best test acc 73.2315\n",
      "epoch 102 | distil loss 0.0006 | tr loss 0.4924 tr acc 86.3081 | val loss 0.7702 val acc 74.0555 | best val acc 74.0555 | best test acc 73.2315\n",
      "epoch 103 | distil loss 0.0006 | tr loss 0.4924 tr acc 86.3181 | val loss 0.7701 val acc 74.0555 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 104 | distil loss 0.0006 | tr loss 0.4923 tr acc 86.3281 | val loss 0.7701 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 105 | distil loss 0.0006 | tr loss 0.4923 tr acc 86.3181 | val loss 0.7701 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 106 | distil loss 0.0006 | tr loss 0.4923 tr acc 86.3181 | val loss 0.7701 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 107 | distil loss 0.0006 | tr loss 0.4922 tr acc 86.3181 | val loss 0.7701 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 108 | distil loss 0.0006 | tr loss 0.4922 tr acc 86.3281 | val loss 0.7701 val acc 74.0153 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 109 | distil loss 0.0006 | tr loss 0.4921 tr acc 86.3281 | val loss 0.7701 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 110 | distil loss 0.0006 | tr loss 0.4921 tr acc 86.3281 | val loss 0.7701 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 111 | distil loss 0.0006 | tr loss 0.4920 tr acc 86.3281 | val loss 0.7701 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 112 | distil loss 0.0006 | tr loss 0.4920 tr acc 86.3181 | val loss 0.7701 val acc 73.9751 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 113 | distil loss 0.0006 | tr loss 0.4920 tr acc 86.3281 | val loss 0.7700 val acc 73.9751 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 114 | distil loss 0.0006 | tr loss 0.4919 tr acc 86.3281 | val loss 0.7700 val acc 73.9751 | best val acc 74.0555 | best test acc 73.2114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 115 | distil loss 0.0006 | tr loss 0.4919 tr acc 86.3281 | val loss 0.7700 val acc 73.9751 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 116 | distil loss 0.0006 | tr loss 0.4918 tr acc 86.3381 | val loss 0.7700 val acc 73.9550 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 117 | distil loss 0.0006 | tr loss 0.4918 tr acc 86.3381 | val loss 0.7700 val acc 73.9550 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 118 | distil loss 0.0006 | tr loss 0.4918 tr acc 86.3281 | val loss 0.7700 val acc 73.9751 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 119 | distil loss 0.0006 | tr loss 0.4917 tr acc 86.3281 | val loss 0.7700 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 120 | distil loss 0.0006 | tr loss 0.4917 tr acc 86.3281 | val loss 0.7700 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 121 | distil loss 0.0006 | tr loss 0.4916 tr acc 86.3281 | val loss 0.7700 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 122 | distil loss 0.0006 | tr loss 0.4916 tr acc 86.3281 | val loss 0.7700 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 123 | distil loss 0.0006 | tr loss 0.4915 tr acc 86.3281 | val loss 0.7700 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 124 | distil loss 0.0006 | tr loss 0.4915 tr acc 86.3281 | val loss 0.7699 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 125 | distil loss 0.0006 | tr loss 0.4915 tr acc 86.3281 | val loss 0.7699 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 126 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.3281 | val loss 0.7699 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 127 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.3381 | val loss 0.7699 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 128 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.3381 | val loss 0.7699 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 129 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.3381 | val loss 0.7699 val acc 74.0354 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 130 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.3381 | val loss 0.7699 val acc 74.0153 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 131 | distil loss 0.0006 | tr loss 0.4912 tr acc 86.3381 | val loss 0.7699 val acc 74.0153 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 132 | distil loss 0.0006 | tr loss 0.4912 tr acc 86.3381 | val loss 0.7699 val acc 74.0153 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 133 | distil loss 0.0006 | tr loss 0.4911 tr acc 86.3381 | val loss 0.7699 val acc 74.0153 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 134 | distil loss 0.0006 | tr loss 0.4911 tr acc 86.3381 | val loss 0.7699 val acc 74.0153 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 135 | distil loss 0.0006 | tr loss 0.4911 tr acc 86.3482 | val loss 0.7698 val acc 74.0153 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 136 | distil loss 0.0006 | tr loss 0.4910 tr acc 86.3582 | val loss 0.7698 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 137 | distil loss 0.0006 | tr loss 0.4910 tr acc 86.3582 | val loss 0.7698 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 138 | distil loss 0.0006 | tr loss 0.4909 tr acc 86.3582 | val loss 0.7698 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 139 | distil loss 0.0006 | tr loss 0.4909 tr acc 86.3582 | val loss 0.7698 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 140 | distil loss 0.0006 | tr loss 0.4909 tr acc 86.3482 | val loss 0.7698 val acc 73.9952 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 141 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3482 | val loss 0.7698 val acc 74.0153 | best val acc 74.0555 | best test acc 73.2114\n",
      "epoch 142 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3482 | val loss 0.7698 val acc 74.0555 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 143 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3482 | val loss 0.7698 val acc 74.0555 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 144 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3482 | val loss 0.7698 val acc 74.0555 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 145 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3482 | val loss 0.7698 val acc 74.0354 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 146 | distil loss 0.0006 | tr loss 0.4906 tr acc 86.3482 | val loss 0.7697 val acc 74.0555 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 147 | distil loss 0.0006 | tr loss 0.4906 tr acc 86.3582 | val loss 0.7697 val acc 74.0354 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 148 | distil loss 0.0006 | tr loss 0.4905 tr acc 86.3582 | val loss 0.7697 val acc 74.0153 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 149 | distil loss 0.0006 | tr loss 0.4905 tr acc 86.3682 | val loss 0.7697 val acc 74.0153 | best val acc 74.0555 | best test acc 73.1913\n",
      "----> Epoch 150 distillation lr 0.000100\n",
      "epoch 150 | distil loss 0.0006 | tr loss 0.4906 tr acc 86.3882 | val loss 0.7697 val acc 73.9550 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 151 | distil loss 0.0006 | tr loss 0.4906 tr acc 86.3882 | val loss 0.7698 val acc 73.8746 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 152 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3982 | val loss 0.7698 val acc 73.8746 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 153 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3882 | val loss 0.7698 val acc 73.8545 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 154 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3882 | val loss 0.7698 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 155 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3682 | val loss 0.7698 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 156 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3682 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 157 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 158 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 159 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 160 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 161 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 162 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 163 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 164 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 165 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 166 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 167 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 168 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 169 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 170 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 171 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 172 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 173 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 174 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 175 | distil loss 0.0006 | tr loss 0.4908 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 176 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 177 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 178 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 179 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 180 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 181 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 182 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 183 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 184 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 185 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 186 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 187 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 188 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 189 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 190 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 191 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 192 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 193 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 194 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 195 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 196 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 197 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 198 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n",
      "epoch 199 | distil loss 0.0006 | tr loss 0.4907 tr acc 86.3782 | val loss 0.7699 val acc 73.8344 | best val acc 74.0555 | best test acc 73.1913\n"
     ]
    }
   ],
   "source": [
    "# train final protected model via knowledge distillation\n",
    "\n",
    "distil_model=PurchaseClassifier().cuda()\n",
    "distil_test_criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "distil_schedule=[60, 90, 150]\n",
    "distil_lr=.1\n",
    "distil_epochs=200\n",
    "\n",
    "distil_best_acc=0\n",
    "best_distil_test_acc=0\n",
    "gamma=.1\n",
    "t_softmax=1\n",
    "\n",
    "for epoch in range(distil_epochs):\n",
    "    if epoch in distil_schedule:\n",
    "        distil_lr *= gamma\n",
    "        print('----> Epoch %d distillation lr %f'%(epoch,distil_lr))\n",
    "\n",
    "    distil_optimizer=optim.SGD(distil_model.parameters(), lr=distil_lr, momentum=0.99, weight_decay=1e-5)\n",
    "\n",
    "    distil_tr_loss = train_pub(ref_data_tensor, distil_label_tensor, ref_label_tensor, distil_model, t_softmax,\n",
    "                               distil_optimizer, batch_size=32, alpha=1)\n",
    "\n",
    "    tr_loss,tr_acc = test(private_data_tensor, private_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "    \n",
    "    val_loss,val_acc = test(val_data_tensor, val_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "\n",
    "    distil_is_best = val_acc >= distil_best_acc\n",
    "\n",
    "    distil_best_acc=max(val_acc, distil_best_acc)\n",
    "\n",
    "    if distil_is_best:\n",
    "        _,best_distil_test_acc = test(te_data_tensor, te_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "\n",
    "    save_checkpoint_global(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': distil_model.state_dict(),\n",
    "            'best_acc': distil_best_acc,\n",
    "            'optimizer': distil_optimizer.state_dict(),\n",
    "        },\n",
    "        distil_is_best,\n",
    "        checkpoint=checkpoint_dir,\n",
    "        filename='protected_model.pth.tar',\n",
    "        best_filename='protected_model_best.pth.tar',\n",
    "    )\n",
    "\n",
    "    print('epoch %d | distil loss %.4f | tr loss %.4f tr acc %.4f | val loss %.4f val acc %.4f | best val acc %.4f | best test acc %.4f'%(epoch,distil_tr_loss,tr_loss,tr_acc,val_loss,val_acc,distil_best_acc,best_distil_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unprotected model: train acc 99.4692 test acc 75.6431\n",
      "epoch 0 | distil loss 0.0383 | tr loss 4.3424 tr acc 3.1951 | val loss 4.3512 val acc 3.0547 | best val acc 3.0547 | best test acc 2.8135\n",
      "epoch 1 | distil loss 0.0305 | tr loss 2.8379 tr acc 23.4575 | val loss 2.8673 val acc 22.1865 | best val acc 22.1865 | best test acc 22.5884\n",
      "epoch 2 | distil loss 0.0174 | tr loss 1.7843 tr acc 48.2973 | val loss 1.8450 val acc 46.6037 | best val acc 46.6037 | best test acc 46.2822\n",
      "epoch 3 | distil loss 0.0100 | tr loss 1.3768 tr acc 58.1130 | val loss 1.4635 val acc 54.8834 | best val acc 54.8834 | best test acc 54.2805\n",
      "epoch 4 | distil loss 0.0072 | tr loss 1.2242 tr acc 61.3181 | val loss 1.3333 val acc 57.2749 | best val acc 57.2749 | best test acc 56.8127\n",
      "epoch 5 | distil loss 0.0061 | tr loss 1.0986 tr acc 64.5833 | val loss 1.2267 val acc 59.1439 | best val acc 59.1439 | best test acc 58.4003\n",
      "epoch 6 | distil loss 0.0055 | tr loss 1.0429 tr acc 65.5148 | val loss 1.1813 val acc 60.2894 | best val acc 60.2894 | best test acc 59.0836\n",
      "epoch 7 | distil loss 0.0055 | tr loss 1.1176 tr acc 61.9992 | val loss 1.2641 val acc 57.5764 | best val acc 60.2894 | best test acc 59.0836\n",
      "epoch 8 | distil loss 0.0050 | tr loss 1.0231 tr acc 65.5048 | val loss 1.1712 val acc 60.4301 | best val acc 60.4301 | best test acc 59.8071\n",
      "epoch 9 | distil loss 0.0044 | tr loss 0.9675 tr acc 67.3578 | val loss 1.1239 val acc 61.7564 | best val acc 61.7564 | best test acc 61.4349\n",
      "epoch 10 | distil loss 0.0040 | tr loss 0.9175 tr acc 68.9704 | val loss 1.0785 val acc 62.9622 | best val acc 62.9622 | best test acc 62.4799\n",
      "epoch 11 | distil loss 0.0040 | tr loss 0.8979 tr acc 69.9018 | val loss 1.0608 val acc 63.6857 | best val acc 63.6857 | best test acc 63.0426\n",
      "epoch 12 | distil loss 0.0036 | tr loss 0.9088 tr acc 69.5312 | val loss 1.0865 val acc 62.2387 | best val acc 63.6857 | best test acc 63.0426\n",
      "epoch 13 | distil loss 0.0033 | tr loss 0.8470 tr acc 71.6847 | val loss 1.0298 val acc 64.3690 | best val acc 64.3690 | best test acc 64.6101\n",
      "epoch 14 | distil loss 0.0035 | tr loss 0.8435 tr acc 72.2055 | val loss 1.0317 val acc 64.2283 | best val acc 64.3690 | best test acc 64.6101\n",
      "epoch 15 | distil loss 0.0031 | tr loss 0.8485 tr acc 71.6046 | val loss 1.0474 val acc 65.0121 | best val acc 65.0121 | best test acc 64.5900\n",
      "epoch 16 | distil loss 0.0029 | tr loss 0.8088 tr acc 72.9968 | val loss 1.0071 val acc 65.4140 | best val acc 65.4140 | best test acc 64.6905\n",
      "epoch 17 | distil loss 0.0028 | tr loss 0.7875 tr acc 73.8482 | val loss 0.9832 val acc 66.3786 | best val acc 66.3786 | best test acc 65.9164\n",
      "epoch 18 | distil loss 0.0033 | tr loss 0.9234 tr acc 68.7500 | val loss 1.1239 val acc 61.4550 | best val acc 66.3786 | best test acc 65.9164\n",
      "epoch 19 | distil loss 0.0031 | tr loss 0.8027 tr acc 73.0369 | val loss 1.0060 val acc 64.9518 | best val acc 66.3786 | best test acc 65.9164\n",
      "epoch 20 | distil loss 0.0030 | tr loss 0.7972 tr acc 72.9667 | val loss 0.9874 val acc 65.9968 | best val acc 66.3786 | best test acc 65.9164\n",
      "epoch 21 | distil loss 0.0029 | tr loss 0.7861 tr acc 73.8582 | val loss 0.9899 val acc 66.5796 | best val acc 66.5796 | best test acc 65.3135\n",
      "epoch 22 | distil loss 0.0023 | tr loss 0.6890 tr acc 78.1651 | val loss 0.8970 val acc 68.9912 | best val acc 68.9912 | best test acc 68.2677\n",
      "epoch 23 | distil loss 0.0023 | tr loss 0.7556 tr acc 74.7296 | val loss 0.9659 val acc 66.7805 | best val acc 68.9912 | best test acc 68.2677\n",
      "epoch 24 | distil loss 0.0023 | tr loss 0.8485 tr acc 71.9952 | val loss 1.0650 val acc 64.2082 | best val acc 68.9912 | best test acc 68.2677\n",
      "epoch 25 | distil loss 0.0022 | tr loss 0.7261 tr acc 76.5825 | val loss 0.9518 val acc 67.0619 | best val acc 68.9912 | best test acc 68.2677\n",
      "epoch 26 | distil loss 0.0020 | tr loss 0.7568 tr acc 75.1202 | val loss 0.9832 val acc 66.6198 | best val acc 68.9912 | best test acc 68.2677\n",
      "epoch 27 | distil loss 0.0022 | tr loss 0.7516 tr acc 75.0200 | val loss 0.9766 val acc 66.7002 | best val acc 68.9912 | best test acc 68.2677\n",
      "epoch 28 | distil loss 0.0021 | tr loss 0.7279 tr acc 75.9415 | val loss 0.9545 val acc 67.5241 | best val acc 68.9912 | best test acc 68.2677\n",
      "epoch 29 | distil loss 0.0023 | tr loss 0.6739 tr acc 78.6558 | val loss 0.8990 val acc 69.9558 | best val acc 69.9558 | best test acc 68.8706\n",
      "epoch 30 | distil loss 0.0023 | tr loss 0.8126 tr acc 72.4459 | val loss 1.0365 val acc 64.6101 | best val acc 69.9558 | best test acc 68.8706\n",
      "epoch 31 | distil loss 0.0030 | tr loss 0.8402 tr acc 72.8866 | val loss 1.0556 val acc 64.3690 | best val acc 69.9558 | best test acc 68.8706\n",
      "epoch 32 | distil loss 0.0020 | tr loss 0.6914 tr acc 77.9046 | val loss 0.9165 val acc 68.7299 | best val acc 69.9558 | best test acc 68.8706\n",
      "epoch 33 | distil loss 0.0017 | tr loss 0.6599 tr acc 79.1667 | val loss 0.8860 val acc 69.6543 | best val acc 69.9558 | best test acc 68.8706\n",
      "epoch 34 | distil loss 0.0017 | tr loss 0.6366 tr acc 79.8878 | val loss 0.8657 val acc 70.1969 | best val acc 70.1969 | best test acc 69.8955\n",
      "epoch 35 | distil loss 0.0019 | tr loss 0.7222 tr acc 76.8630 | val loss 0.9581 val acc 67.5442 | best val acc 70.1969 | best test acc 69.8955\n",
      "epoch 36 | distil loss 0.0024 | tr loss 0.8258 tr acc 72.4259 | val loss 1.0493 val acc 65.2331 | best val acc 70.1969 | best test acc 69.8955\n",
      "epoch 37 | distil loss 0.0021 | tr loss 0.6542 tr acc 78.9563 | val loss 0.8845 val acc 69.9156 | best val acc 70.1969 | best test acc 69.8955\n",
      "epoch 38 | distil loss 0.0022 | tr loss 0.7483 tr acc 75.2204 | val loss 0.9810 val acc 66.2982 | best val acc 70.1969 | best test acc 69.8955\n",
      "epoch 39 | distil loss 0.0020 | tr loss 0.6915 tr acc 77.6242 | val loss 0.9342 val acc 67.8256 | best val acc 70.1969 | best test acc 69.8955\n",
      "epoch 40 | distil loss 0.0018 | tr loss 0.6259 tr acc 80.4187 | val loss 0.8608 val acc 70.5788 | best val acc 70.5788 | best test acc 70.1768\n",
      "epoch 41 | distil loss 0.0015 | tr loss 0.6433 tr acc 79.3169 | val loss 0.8891 val acc 69.7950 | best val acc 70.5788 | best test acc 70.1768\n",
      "epoch 42 | distil loss 0.0016 | tr loss 0.5965 tr acc 81.9611 | val loss 0.8448 val acc 70.7797 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 43 | distil loss 0.0015 | tr loss 0.6351 tr acc 79.7977 | val loss 0.8895 val acc 69.0715 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 44 | distil loss 0.0015 | tr loss 0.7071 tr acc 76.7228 | val loss 0.9595 val acc 67.8055 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 45 | distil loss 0.0016 | tr loss 0.6417 tr acc 79.5172 | val loss 0.8904 val acc 69.1921 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 46 | distil loss 0.0017 | tr loss 0.6375 tr acc 79.1867 | val loss 0.8886 val acc 69.5137 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 47 | distil loss 0.0018 | tr loss 0.7779 tr acc 74.2087 | val loss 1.0285 val acc 65.7355 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 48 | distil loss 0.0019 | tr loss 0.6944 tr acc 76.8730 | val loss 0.9450 val acc 67.5241 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 49 | distil loss 0.0014 | tr loss 0.5892 tr acc 81.8610 | val loss 0.8445 val acc 70.7395 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 50 | distil loss 0.0013 | tr loss 0.6355 tr acc 79.7676 | val loss 0.8950 val acc 69.2725 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 51 | distil loss 0.0014 | tr loss 0.7079 tr acc 77.0232 | val loss 0.9629 val acc 67.4839 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 52 | distil loss 0.0014 | tr loss 0.6962 tr acc 77.1735 | val loss 0.9541 val acc 67.6849 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 53 | distil loss 0.0014 | tr loss 0.6162 tr acc 80.9896 | val loss 0.8735 val acc 70.0362 | best val acc 70.7797 | best test acc 70.7395\n",
      "epoch 54 | distil loss 0.0016 | tr loss 0.5902 tr acc 81.6607 | val loss 0.8488 val acc 71.0611 | best val acc 71.0611 | best test acc 71.0611\n",
      "epoch 55 | distil loss 0.0019 | tr loss 0.6669 tr acc 78.4856 | val loss 0.9263 val acc 68.0265 | best val acc 71.0611 | best test acc 71.0611\n",
      "epoch 56 | distil loss 0.0016 | tr loss 0.6022 tr acc 81.1198 | val loss 0.8556 val acc 70.6190 | best val acc 71.0611 | best test acc 71.0611\n",
      "epoch 57 | distil loss 0.0014 | tr loss 0.5921 tr acc 81.5204 | val loss 0.8609 val acc 71.0008 | best val acc 71.0611 | best test acc 71.0611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 | distil loss 0.0015 | tr loss 0.6099 tr acc 80.6891 | val loss 0.8821 val acc 70.1969 | best val acc 71.0611 | best test acc 71.0611\n",
      "epoch 59 | distil loss 0.0016 | tr loss 0.6243 tr acc 80.1082 | val loss 0.8899 val acc 70.1166 | best val acc 71.0611 | best test acc 71.0611\n",
      "----> Epoch 60 distillation lr 0.010000\n",
      "epoch 60 | distil loss 0.0011 | tr loss 0.5279 tr acc 84.5853 | val loss 0.7915 val acc 73.1913 | best val acc 73.1913 | best test acc 72.4879\n",
      "epoch 61 | distil loss 0.0008 | tr loss 0.5202 tr acc 84.8858 | val loss 0.7852 val acc 73.3521 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 62 | distil loss 0.0008 | tr loss 0.5181 tr acc 84.9860 | val loss 0.7837 val acc 73.3320 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 63 | distil loss 0.0008 | tr loss 0.5162 tr acc 85.1162 | val loss 0.7822 val acc 73.2315 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 64 | distil loss 0.0008 | tr loss 0.5146 tr acc 85.2163 | val loss 0.7812 val acc 73.2315 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 65 | distil loss 0.0007 | tr loss 0.5132 tr acc 85.3165 | val loss 0.7802 val acc 73.2516 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 66 | distil loss 0.0007 | tr loss 0.5119 tr acc 85.4067 | val loss 0.7794 val acc 73.2717 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 67 | distil loss 0.0007 | tr loss 0.5108 tr acc 85.4067 | val loss 0.7787 val acc 73.2918 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 68 | distil loss 0.0007 | tr loss 0.5097 tr acc 85.4567 | val loss 0.7781 val acc 73.2918 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 69 | distil loss 0.0007 | tr loss 0.5087 tr acc 85.5168 | val loss 0.7775 val acc 73.3320 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 70 | distil loss 0.0007 | tr loss 0.5078 tr acc 85.5369 | val loss 0.7770 val acc 73.3320 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 71 | distil loss 0.0007 | tr loss 0.5070 tr acc 85.5469 | val loss 0.7765 val acc 73.2516 | best val acc 73.3521 | best test acc 72.6286\n",
      "epoch 72 | distil loss 0.0007 | tr loss 0.5062 tr acc 85.5469 | val loss 0.7761 val acc 73.3521 | best val acc 73.3521 | best test acc 72.9100\n",
      "epoch 73 | distil loss 0.0007 | tr loss 0.5054 tr acc 85.5869 | val loss 0.7756 val acc 73.4325 | best val acc 73.4325 | best test acc 72.9100\n",
      "epoch 74 | distil loss 0.0007 | tr loss 0.5047 tr acc 85.5970 | val loss 0.7753 val acc 73.3521 | best val acc 73.4325 | best test acc 72.9100\n",
      "epoch 75 | distil loss 0.0007 | tr loss 0.5040 tr acc 85.6671 | val loss 0.7749 val acc 73.4124 | best val acc 73.4325 | best test acc 72.9100\n",
      "epoch 76 | distil loss 0.0007 | tr loss 0.5033 tr acc 85.6671 | val loss 0.7746 val acc 73.4124 | best val acc 73.4325 | best test acc 72.9100\n",
      "epoch 77 | distil loss 0.0007 | tr loss 0.5027 tr acc 85.7171 | val loss 0.7742 val acc 73.4124 | best val acc 73.4325 | best test acc 72.9100\n",
      "epoch 78 | distil loss 0.0007 | tr loss 0.5020 tr acc 85.7272 | val loss 0.7739 val acc 73.4727 | best val acc 73.4727 | best test acc 72.8296\n",
      "epoch 79 | distil loss 0.0007 | tr loss 0.5015 tr acc 85.7472 | val loss 0.7736 val acc 73.4928 | best val acc 73.4928 | best test acc 72.8095\n",
      "epoch 80 | distil loss 0.0007 | tr loss 0.5009 tr acc 85.7171 | val loss 0.7733 val acc 73.5129 | best val acc 73.5129 | best test acc 72.8497\n",
      "epoch 81 | distil loss 0.0006 | tr loss 0.5003 tr acc 85.7272 | val loss 0.7731 val acc 73.5330 | best val acc 73.5330 | best test acc 72.9100\n",
      "epoch 82 | distil loss 0.0006 | tr loss 0.4998 tr acc 85.7472 | val loss 0.7728 val acc 73.4928 | best val acc 73.5330 | best test acc 72.9100\n",
      "epoch 83 | distil loss 0.0006 | tr loss 0.4993 tr acc 85.7772 | val loss 0.7726 val acc 73.4526 | best val acc 73.5330 | best test acc 72.9100\n",
      "epoch 84 | distil loss 0.0006 | tr loss 0.4988 tr acc 85.7672 | val loss 0.7723 val acc 73.4526 | best val acc 73.5330 | best test acc 72.9100\n",
      "epoch 85 | distil loss 0.0006 | tr loss 0.4983 tr acc 85.7873 | val loss 0.7721 val acc 73.4325 | best val acc 73.5330 | best test acc 72.9100\n",
      "epoch 86 | distil loss 0.0006 | tr loss 0.4978 tr acc 85.7873 | val loss 0.7719 val acc 73.4526 | best val acc 73.5330 | best test acc 72.9100\n",
      "epoch 87 | distil loss 0.0006 | tr loss 0.4974 tr acc 85.7672 | val loss 0.7717 val acc 73.4325 | best val acc 73.5330 | best test acc 72.9100\n",
      "epoch 88 | distil loss 0.0006 | tr loss 0.4969 tr acc 85.7772 | val loss 0.7714 val acc 73.4526 | best val acc 73.5330 | best test acc 72.9100\n",
      "epoch 89 | distil loss 0.0006 | tr loss 0.4965 tr acc 85.7873 | val loss 0.7712 val acc 73.5129 | best val acc 73.5330 | best test acc 72.9100\n",
      "----> Epoch 90 distillation lr 0.001000\n",
      "epoch 90 | distil loss 0.0006 | tr loss 0.4933 tr acc 85.9275 | val loss 0.7689 val acc 73.5330 | best val acc 73.5330 | best test acc 73.2516\n",
      "epoch 91 | distil loss 0.0006 | tr loss 0.4930 tr acc 85.9776 | val loss 0.7686 val acc 73.5531 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 92 | distil loss 0.0006 | tr loss 0.4929 tr acc 85.9475 | val loss 0.7686 val acc 73.5330 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 93 | distil loss 0.0006 | tr loss 0.4929 tr acc 85.9776 | val loss 0.7685 val acc 73.5129 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 94 | distil loss 0.0006 | tr loss 0.4929 tr acc 85.9675 | val loss 0.7685 val acc 73.4928 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 95 | distil loss 0.0006 | tr loss 0.4928 tr acc 85.9575 | val loss 0.7685 val acc 73.4727 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 96 | distil loss 0.0006 | tr loss 0.4928 tr acc 85.9575 | val loss 0.7685 val acc 73.4124 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 97 | distil loss 0.0006 | tr loss 0.4928 tr acc 85.9675 | val loss 0.7685 val acc 73.4124 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 98 | distil loss 0.0006 | tr loss 0.4927 tr acc 85.9575 | val loss 0.7685 val acc 73.4124 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 99 | distil loss 0.0006 | tr loss 0.4927 tr acc 85.9575 | val loss 0.7685 val acc 73.4325 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 100 | distil loss 0.0006 | tr loss 0.4927 tr acc 85.9575 | val loss 0.7685 val acc 73.4325 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 101 | distil loss 0.0006 | tr loss 0.4926 tr acc 85.9575 | val loss 0.7685 val acc 73.4727 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 102 | distil loss 0.0006 | tr loss 0.4926 tr acc 85.9575 | val loss 0.7684 val acc 73.4928 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 103 | distil loss 0.0006 | tr loss 0.4925 tr acc 85.9776 | val loss 0.7684 val acc 73.4928 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 104 | distil loss 0.0006 | tr loss 0.4925 tr acc 85.9776 | val loss 0.7684 val acc 73.4928 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 105 | distil loss 0.0006 | tr loss 0.4925 tr acc 85.9876 | val loss 0.7684 val acc 73.4928 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 106 | distil loss 0.0006 | tr loss 0.4924 tr acc 85.9876 | val loss 0.7684 val acc 73.5129 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 107 | distil loss 0.0006 | tr loss 0.4924 tr acc 85.9876 | val loss 0.7684 val acc 73.5330 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 108 | distil loss 0.0006 | tr loss 0.4924 tr acc 85.9876 | val loss 0.7683 val acc 73.5330 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 109 | distil loss 0.0006 | tr loss 0.4923 tr acc 85.9776 | val loss 0.7683 val acc 73.5330 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 110 | distil loss 0.0006 | tr loss 0.4923 tr acc 85.9776 | val loss 0.7683 val acc 73.5330 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 111 | distil loss 0.0006 | tr loss 0.4922 tr acc 85.9776 | val loss 0.7683 val acc 73.5330 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 112 | distil loss 0.0006 | tr loss 0.4922 tr acc 85.9876 | val loss 0.7683 val acc 73.5330 | best val acc 73.5531 | best test acc 73.1511\n",
      "epoch 113 | distil loss 0.0006 | tr loss 0.4922 tr acc 85.9976 | val loss 0.7683 val acc 73.5531 | best val acc 73.5531 | best test acc 73.1913\n",
      "epoch 114 | distil loss 0.0006 | tr loss 0.4921 tr acc 85.9976 | val loss 0.7682 val acc 73.5531 | best val acc 73.5531 | best test acc 73.1913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 115 | distil loss 0.0006 | tr loss 0.4921 tr acc 85.9976 | val loss 0.7682 val acc 73.5732 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 116 | distil loss 0.0006 | tr loss 0.4921 tr acc 86.0076 | val loss 0.7682 val acc 73.5732 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 117 | distil loss 0.0006 | tr loss 0.4920 tr acc 86.0076 | val loss 0.7682 val acc 73.5732 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 118 | distil loss 0.0006 | tr loss 0.4920 tr acc 86.0076 | val loss 0.7682 val acc 73.5732 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 119 | distil loss 0.0006 | tr loss 0.4920 tr acc 86.0076 | val loss 0.7682 val acc 73.5330 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 120 | distil loss 0.0006 | tr loss 0.4919 tr acc 85.9976 | val loss 0.7682 val acc 73.5330 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 121 | distil loss 0.0006 | tr loss 0.4919 tr acc 86.0176 | val loss 0.7681 val acc 73.5330 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 122 | distil loss 0.0006 | tr loss 0.4919 tr acc 86.0176 | val loss 0.7681 val acc 73.5531 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 123 | distil loss 0.0006 | tr loss 0.4918 tr acc 86.0176 | val loss 0.7681 val acc 73.5330 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 124 | distil loss 0.0006 | tr loss 0.4918 tr acc 86.0276 | val loss 0.7681 val acc 73.5531 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 125 | distil loss 0.0006 | tr loss 0.4917 tr acc 86.0276 | val loss 0.7681 val acc 73.5531 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 126 | distil loss 0.0006 | tr loss 0.4917 tr acc 86.0377 | val loss 0.7681 val acc 73.5330 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 127 | distil loss 0.0006 | tr loss 0.4917 tr acc 86.0577 | val loss 0.7680 val acc 73.5531 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 128 | distil loss 0.0006 | tr loss 0.4916 tr acc 86.0777 | val loss 0.7680 val acc 73.5531 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 129 | distil loss 0.0006 | tr loss 0.4916 tr acc 86.0777 | val loss 0.7680 val acc 73.5531 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 130 | distil loss 0.0006 | tr loss 0.4916 tr acc 86.0777 | val loss 0.7680 val acc 73.5129 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 131 | distil loss 0.0006 | tr loss 0.4915 tr acc 86.0777 | val loss 0.7680 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 132 | distil loss 0.0006 | tr loss 0.4915 tr acc 86.0777 | val loss 0.7680 val acc 73.5129 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 133 | distil loss 0.0006 | tr loss 0.4915 tr acc 86.0677 | val loss 0.7680 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 134 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0577 | val loss 0.7679 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 135 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0577 | val loss 0.7679 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 136 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0577 | val loss 0.7679 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 137 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0677 | val loss 0.7679 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 138 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0877 | val loss 0.7679 val acc 73.5129 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 139 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0978 | val loss 0.7679 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 140 | distil loss 0.0006 | tr loss 0.4912 tr acc 86.0978 | val loss 0.7679 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 141 | distil loss 0.0006 | tr loss 0.4912 tr acc 86.0978 | val loss 0.7678 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 142 | distil loss 0.0006 | tr loss 0.4912 tr acc 86.0978 | val loss 0.7678 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 143 | distil loss 0.0006 | tr loss 0.4911 tr acc 86.0978 | val loss 0.7678 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 144 | distil loss 0.0006 | tr loss 0.4911 tr acc 86.0978 | val loss 0.7678 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 145 | distil loss 0.0006 | tr loss 0.4911 tr acc 86.0978 | val loss 0.7678 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 146 | distil loss 0.0006 | tr loss 0.4910 tr acc 86.0777 | val loss 0.7678 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 147 | distil loss 0.0006 | tr loss 0.4910 tr acc 86.0777 | val loss 0.7678 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 148 | distil loss 0.0006 | tr loss 0.4910 tr acc 86.0777 | val loss 0.7677 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 149 | distil loss 0.0006 | tr loss 0.4909 tr acc 86.0978 | val loss 0.7677 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "----> Epoch 150 distillation lr 0.000100\n",
      "epoch 150 | distil loss 0.0006 | tr loss 0.4911 tr acc 86.0877 | val loss 0.7678 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 151 | distil loss 0.0006 | tr loss 0.4912 tr acc 86.0577 | val loss 0.7679 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 152 | distil loss 0.0006 | tr loss 0.4912 tr acc 86.0577 | val loss 0.7679 val acc 73.4928 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 153 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4325 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 154 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0176 | val loss 0.7680 val acc 73.4325 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 155 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0276 | val loss 0.7680 val acc 73.4325 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 156 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4325 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 157 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4325 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 158 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4325 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 159 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4325 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 160 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 161 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4325 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 162 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 163 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 164 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 165 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 166 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 167 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 168 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 169 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 170 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0176 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 171 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0276 | val loss 0.7680 val acc 73.4526 | best val acc 73.5732 | best test acc 73.2114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 172 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 173 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 174 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 175 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 176 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 177 | distil loss 0.0006 | tr loss 0.4914 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 178 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 179 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 180 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 181 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 182 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 183 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 184 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 185 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 186 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 187 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 188 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 189 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 190 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 191 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 192 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 193 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 194 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 195 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 196 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0276 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 197 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 198 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n",
      "epoch 199 | distil loss 0.0006 | tr loss 0.4913 tr acc 86.0377 | val loss 0.7680 val acc 73.4727 | best val acc 73.5732 | best test acc 73.2114\n"
     ]
    }
   ],
   "source": [
    "# distil the knowledge of the unprotected model in the ref data\n",
    "checkpoint_dir='./dmp'\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "best_model=PurchaseClassifier().cuda()\n",
    "\n",
    "resume_best=checkpoint_dir+'/unprotected_model_best.pth.tar'\n",
    "\n",
    "assert os.path.isfile(resume_best), 'Error: no checkpoint directory found for best model'\n",
    "checkpoint = os.path.dirname(resume_best)\n",
    "checkpoint = torch.load(resume_best)\n",
    "best_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "_,best_test = test(te_data_tensor, te_label_tensor, best_model, criterion, use_cuda)\n",
    "_,best_train = test(private_data_tensor, private_label_tensor, best_model, criterion, use_cuda)\n",
    "print('unprotected model: train acc %.4f test acc %.4f'%(best_train, best_test))\n",
    "\n",
    "batch_size=100\n",
    "all_outputs=[]\n",
    "\n",
    "len_t = len(ref_data_tensor)//batch_size\n",
    "\n",
    "for ind in range(len_t):\n",
    "    inputs = ref_data_tensor[ind*batch_size:(ind+1)*batch_size]\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    inputs = torch.autograd.Variable(inputs)\n",
    "    outputs,_,_ = best_model(inputs)\n",
    "    all_outputs.append(outputs.data.cpu().numpy())\n",
    "\n",
    "if len(ref_data_tensor)%batch_size:\n",
    "    inputs=ref_data_tensor[-(len(target_ref_data_tensor)%batch_size):]\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    inputs = torch.autograd.Variable(inputs)\n",
    "    outputs,_,_ = best_model(inputs)\n",
    "    all_outputs.append(outputs.data.cpu().numpy())\n",
    "\n",
    "final_outputs=np.concatenate(all_outputs)\n",
    "distil_label_tensor=(torch.from_numpy(final_outputs).type(torch.FloatTensor))\n",
    "\n",
    "\n",
    "\n",
    "# train final protected model via knowledge distillation\n",
    "\n",
    "distil_model=PurchaseClassifier().cuda()\n",
    "distil_test_criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "distil_schedule=[60, 90, 150]\n",
    "distil_lr=.1\n",
    "distil_epochs=200\n",
    "\n",
    "distil_best_acc=0\n",
    "best_distil_test_acc=0\n",
    "gamma=.1\n",
    "t_softmax=1\n",
    "\n",
    "for epoch in range(distil_epochs):\n",
    "    if epoch in distil_schedule:\n",
    "        distil_lr *= gamma\n",
    "        print('----> Epoch %d distillation lr %f'%(epoch,distil_lr))\n",
    "\n",
    "    distil_optimizer=optim.SGD(distil_model.parameters(), lr=distil_lr, momentum=0.99, weight_decay=1e-5)\n",
    "\n",
    "    distil_tr_loss = train_pub(ref_data_tensor, distil_label_tensor, ref_label_tensor, distil_model, t_softmax,\n",
    "                               distil_optimizer, batch_size=32, alpha=1)\n",
    "\n",
    "    tr_loss,tr_acc = test(private_data_tensor, private_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "    \n",
    "    val_loss,val_acc = test(val_data_tensor, val_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "\n",
    "    distil_is_best = val_acc >= distil_best_acc\n",
    "\n",
    "    distil_best_acc=max(val_acc, distil_best_acc)\n",
    "\n",
    "    if distil_is_best:\n",
    "        _,best_distil_test_acc = test(te_data_tensor, te_label_tensor, distil_model, distil_test_criterion, use_cuda)\n",
    "\n",
    "    save_checkpoint_global(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': distil_model.state_dict(),\n",
    "            'best_acc': distil_best_acc,\n",
    "            'optimizer': distil_optimizer.state_dict(),\n",
    "        },\n",
    "        distil_is_best,\n",
    "        checkpoint=checkpoint_dir,\n",
    "        filename='protected_model.pth.tar',\n",
    "        best_filename='protected_model_best.pth.tar',\n",
    "    )\n",
    "\n",
    "    print('epoch %d | distil loss %.4f | tr loss %.4f tr acc %.4f | val loss %.4f val acc %.4f | best val acc %.4f | best test acc %.4f'%(epoch,distil_tr_loss,tr_loss,tr_acc,val_loss,val_acc,distil_best_acc,best_distil_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceAttack_BB(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        super(InferenceAttack_BB, self).__init__()\n",
    "        \n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(100,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        self.labels=nn.Sequential(\n",
    "           nn.Linear(num_classes,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        self.loss=nn.Sequential(\n",
    "           nn.Linear(1,num_classes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_classes,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "        self.combine=nn.Sequential(\n",
    "            nn.Linear(64*3,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            )\n",
    "\n",
    "        for key in self.state_dict():\n",
    "            # print (key)\n",
    "            if key.split('.')[-1] == 'weight':    \n",
    "                nn.init.normal_(self.state_dict()[key], std=0.01)\n",
    "                \n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "        self.output= nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x1,one_hot_labels,loss):\n",
    "\n",
    "        out_x1 = self.features(x1)\n",
    "        \n",
    "        out_l = self.labels(one_hot_labels)\n",
    "        \n",
    "        out_loss= self.loss(loss)\n",
    "\n",
    "        is_member =self.combine( torch.cat((out_x1,out_l,out_loss),1))\n",
    "        \n",
    "        return self.output(is_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_bb(train_data, labels, attack_data, attack_label, model, inference_model, classifier_criterion, classifier_criterion_noreduct, criterion_attck, classifier_optimizer,\n",
    "              optimizer, epoch, use_cuda, num_batchs=1000, is_train=False, batch_size=64):\n",
    "    global best_acc\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    inference_model.eval()\n",
    "    \n",
    "    skip_batch=0\n",
    "    \n",
    "    if is_train:\n",
    "        inference_model.train()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = batch_size//2\n",
    "    #len_t =  min((len(attack_data)//batch_size) ,(len(train_data)//batch_size))-1\n",
    "    \n",
    "    len_t = len(train_data)//batch_size\n",
    "    if len(train_data)%batch_size:\n",
    "        len_t += 1\n",
    "\n",
    "    for ind in range(skip_batch, len_t):\n",
    "\n",
    "        if ind >= skip_batch+num_batchs:\n",
    "            break\n",
    "        \n",
    "        if ind > (len(attack_data)//batch_size)-1 :\n",
    "            ind = ind % (len(attack_data)//batch_size)\n",
    "\n",
    "        tr_input = train_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        tr_target = labels[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        if ind > (len(attack_data)//batch_size)-1 :\n",
    "            ind=ind%(len(attack_data)//batch_size)\n",
    "\n",
    "        te_input = attack_data[ind*batch_size:(ind+1)*batch_size]\n",
    "        te_target = attack_label[ind*batch_size:(ind+1)*batch_size]\n",
    "        \n",
    "        tr_input, tr_target = tr_input.cuda(), tr_target.cuda()\n",
    "        te_input , te_target = te_input.cuda(), te_target.cuda()\n",
    "\n",
    "        v_tr_input, v_tr_target = torch.autograd.Variable(tr_input), torch.autograd.Variable(tr_target)\n",
    "        v_te_input, v_te_target = torch.autograd.Variable(te_input), torch.autograd.Variable(te_target)\n",
    "\n",
    "        \n",
    "        # compute output\n",
    "        model_input = torch.cat((v_tr_input, v_te_input))\n",
    "        \n",
    "        pred_outputs, _, _ = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        one_hot_tr = torch.from_numpy(np.zeros(pred_outputs.size())).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "\n",
    "        loss_= classifier_criterion_noreduct(pred_outputs, infer_input).view([-1,1])\n",
    "        #torch.autograd.Variable(torch.from_numpy(c.view([-1,1]).data.cpu().numpy()).cuda())\n",
    "\n",
    "        preds = torch.autograd.Variable(torch.from_numpy(pred_outputs.data.cpu().numpy()).cuda())\n",
    "        member_output = inference_model(pred_outputs, infer_input_one_hot, loss_)\n",
    "\n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        loss = criterion_attck(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.item(), model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # plot progress\n",
    "        if False and ind%10==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=ind ,\n",
    "                    size=len_t,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Evaluating DMP model against BB meminf attack  ================\n",
      "\n",
      "Private model | train acc 86.0076 | val acc 73.5732 | test acc 73.2114\n",
      "protected_model | epoch 0 | attack val acc 0.5641 | best val acc 0.5641 | test acc 0.5742\n",
      "protected_model | epoch 1 | attack val acc 0.5649 | best val acc 0.5649 | test acc 0.5837\n",
      "protected_model | epoch 2 | attack val acc 0.5690 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 3 | attack val acc 0.5676 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 4 | attack val acc 0.5653 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 5 | attack val acc 0.5680 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 6 | attack val acc 0.5653 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 7 | attack val acc 0.5674 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 8 | attack val acc 0.5676 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 9 | attack val acc 0.5645 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 10 | attack val acc 0.5621 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 11 | attack val acc 0.5615 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 12 | attack val acc 0.5627 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 13 | attack val acc 0.5665 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 14 | attack val acc 0.5585 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 15 | attack val acc 0.5625 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 16 | attack val acc 0.5581 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 17 | attack val acc 0.5619 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 18 | attack val acc 0.5651 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 19 | attack val acc 0.5581 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 20 | attack val acc 0.5562 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 21 | attack val acc 0.5556 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 22 | attack val acc 0.5619 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 23 | attack val acc 0.5589 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 24 | attack val acc 0.5554 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 25 | attack val acc 0.5635 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 26 | attack val acc 0.5617 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 27 | attack val acc 0.5647 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 28 | attack val acc 0.5570 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 29 | attack val acc 0.5609 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 30 | attack val acc 0.5564 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 31 | attack val acc 0.5623 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 32 | attack val acc 0.5587 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 33 | attack val acc 0.5578 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 34 | attack val acc 0.5564 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 35 | attack val acc 0.5583 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 36 | attack val acc 0.5556 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 37 | attack val acc 0.5560 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 38 | attack val acc 0.5576 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 39 | attack val acc 0.5607 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 40 | attack val acc 0.5597 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 41 | attack val acc 0.5548 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 42 | attack val acc 0.5481 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 43 | attack val acc 0.5514 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 44 | attack val acc 0.5504 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 45 | attack val acc 0.5500 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 46 | attack val acc 0.5526 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 47 | attack val acc 0.5546 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 48 | attack val acc 0.5558 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 49 | attack val acc 0.5560 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 50 | attack val acc 0.5603 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 51 | attack val acc 0.5526 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 52 | attack val acc 0.5508 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 53 | attack val acc 0.5477 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 54 | attack val acc 0.5475 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 55 | attack val acc 0.5522 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 56 | attack val acc 0.5556 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 57 | attack val acc 0.5487 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 58 | attack val acc 0.5427 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 59 | attack val acc 0.5467 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 60 | attack val acc 0.5506 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 61 | attack val acc 0.5445 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 62 | attack val acc 0.5504 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 63 | attack val acc 0.5427 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 64 | attack val acc 0.5437 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 65 | attack val acc 0.5435 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 66 | attack val acc 0.5423 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 67 | attack val acc 0.5307 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 68 | attack val acc 0.5350 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 69 | attack val acc 0.5403 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 70 | attack val acc 0.5330 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 71 | attack val acc 0.5370 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 72 | attack val acc 0.5348 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 73 | attack val acc 0.5356 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 74 | attack val acc 0.5336 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 75 | attack val acc 0.5382 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 76 | attack val acc 0.5449 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 77 | attack val acc 0.5370 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 78 | attack val acc 0.5435 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 79 | attack val acc 0.5402 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 80 | attack val acc 0.5324 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 81 | attack val acc 0.5405 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 82 | attack val acc 0.5372 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 83 | attack val acc 0.5384 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 84 | attack val acc 0.5409 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 85 | attack val acc 0.5370 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 86 | attack val acc 0.5394 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 87 | attack val acc 0.5372 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 88 | attack val acc 0.5417 | best val acc 0.5690 | test acc 0.5916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protected_model | epoch 89 | attack val acc 0.5386 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 90 | attack val acc 0.5380 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 91 | attack val acc 0.5326 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 92 | attack val acc 0.5301 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 93 | attack val acc 0.5362 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 94 | attack val acc 0.5334 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 95 | attack val acc 0.5376 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 96 | attack val acc 0.5352 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 97 | attack val acc 0.5370 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 98 | attack val acc 0.5342 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 99 | attack val acc 0.5318 | best val acc 0.5690 | test acc 0.5916\n",
      "Epoch 100 Local lr 0.000050\n",
      "protected_model | epoch 100 | attack val acc 0.5279 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 101 | attack val acc 0.5229 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 102 | attack val acc 0.5231 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 103 | attack val acc 0.5233 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 104 | attack val acc 0.5225 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 105 | attack val acc 0.5229 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 106 | attack val acc 0.5235 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 107 | attack val acc 0.5227 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 108 | attack val acc 0.5235 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 109 | attack val acc 0.5222 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 110 | attack val acc 0.5227 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 111 | attack val acc 0.5220 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 112 | attack val acc 0.5216 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 113 | attack val acc 0.5212 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 114 | attack val acc 0.5210 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 115 | attack val acc 0.5216 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 116 | attack val acc 0.5212 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 117 | attack val acc 0.5196 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 118 | attack val acc 0.5192 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 119 | attack val acc 0.5188 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 120 | attack val acc 0.5174 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 121 | attack val acc 0.5184 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 122 | attack val acc 0.5182 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 123 | attack val acc 0.5170 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 124 | attack val acc 0.5174 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 125 | attack val acc 0.5198 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 126 | attack val acc 0.5188 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 127 | attack val acc 0.5152 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 128 | attack val acc 0.5184 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 129 | attack val acc 0.5204 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 130 | attack val acc 0.5208 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 131 | attack val acc 0.5192 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 132 | attack val acc 0.5204 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 133 | attack val acc 0.5225 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 134 | attack val acc 0.5222 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 135 | attack val acc 0.5225 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 136 | attack val acc 0.5214 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 137 | attack val acc 0.5198 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 138 | attack val acc 0.5222 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 139 | attack val acc 0.5198 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 140 | attack val acc 0.5198 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 141 | attack val acc 0.5225 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 142 | attack val acc 0.5182 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 143 | attack val acc 0.5206 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 144 | attack val acc 0.5223 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 145 | attack val acc 0.5210 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 146 | attack val acc 0.5198 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 147 | attack val acc 0.5223 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 148 | attack val acc 0.5200 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 149 | attack val acc 0.5220 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 150 | attack val acc 0.5229 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 151 | attack val acc 0.5218 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 152 | attack val acc 0.5212 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 153 | attack val acc 0.5216 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 154 | attack val acc 0.5200 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 155 | attack val acc 0.5194 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 156 | attack val acc 0.5208 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 157 | attack val acc 0.5210 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 158 | attack val acc 0.5223 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 159 | attack val acc 0.5202 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 160 | attack val acc 0.5220 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 161 | attack val acc 0.5208 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 162 | attack val acc 0.5218 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 163 | attack val acc 0.5208 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 164 | attack val acc 0.5176 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 165 | attack val acc 0.5204 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 166 | attack val acc 0.5202 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 167 | attack val acc 0.5194 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 168 | attack val acc 0.5190 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 169 | attack val acc 0.5200 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 170 | attack val acc 0.5233 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 171 | attack val acc 0.5212 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 172 | attack val acc 0.5180 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 173 | attack val acc 0.5184 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 174 | attack val acc 0.5186 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 175 | attack val acc 0.5190 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 176 | attack val acc 0.5206 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 177 | attack val acc 0.5190 | best val acc 0.5690 | test acc 0.5916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protected_model | epoch 178 | attack val acc 0.5188 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 179 | attack val acc 0.5206 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 180 | attack val acc 0.5182 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 181 | attack val acc 0.5176 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 182 | attack val acc 0.5216 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 183 | attack val acc 0.5198 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 184 | attack val acc 0.5235 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 185 | attack val acc 0.5227 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 186 | attack val acc 0.5206 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 187 | attack val acc 0.5202 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 188 | attack val acc 0.5174 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 189 | attack val acc 0.5182 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 190 | attack val acc 0.5210 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 191 | attack val acc 0.5180 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 192 | attack val acc 0.5186 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 193 | attack val acc 0.5223 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 194 | attack val acc 0.5176 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 195 | attack val acc 0.5182 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 196 | attack val acc 0.5218 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 197 | attack val acc 0.5222 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 198 | attack val acc 0.5206 | best val acc 0.5690 | test acc 0.5916\n",
      "protected_model | epoch 199 | attack val acc 0.5227 | best val acc 0.5690 | test acc 0.5916\n"
     ]
    }
   ],
   "source": [
    "at_lr=0.0005\n",
    "at_schedule=[100]\n",
    "at_gamma=0.1\n",
    "n_classes=100\n",
    "criterion_classifier = nn.CrossEntropyLoss(reduction='none')\n",
    "attack_criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_at_val_acc=0\n",
    "best_at_test_acc=0\n",
    "attack_epochs=200\n",
    "\n",
    "print('\\n================ Evaluating DMP model against BB meminf attack  ================\\n')\n",
    "\n",
    "resume_best=checkpoint_dir+'/protected_model_best.pth.tar'\n",
    "\n",
    "attack_model = InferenceAttack_BB(n_classes)\n",
    "attack_model = attack_model.cuda()\n",
    "attack_optimizer = optim.Adam(attack_model.parameters(),lr=at_lr)\n",
    "\n",
    "best_model=PurchaseClassifier()\n",
    "best_model=best_model.cuda()\n",
    "best_opt=optim.Adam(best_model.parameters(), lr=user_lr)\n",
    "\n",
    "assert os.path.isfile(resume_best), 'Error: no checkpoint directory %s found for best model'%resume_best\n",
    "checkpoint = os.path.dirname(resume_best)\n",
    "checkpoint = torch.load(resume_best)\n",
    "best_model.load_state_dict(checkpoint['state_dict'])\n",
    "best_opt.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "_, check_test_acc = test(te_data_tensor,te_label_tensor,best_model,criterion,use_cuda)\n",
    "_, check_val_acc = test(val_data_tensor,val_label_tensor,best_model,criterion,use_cuda)\n",
    "_, check_train_acc = test(private_data_tensor,private_label_tensor,best_model,criterion,use_cuda)\n",
    "\n",
    "print('Private model | train acc %.4f | val acc %.4f | test acc %.4f'%(check_train_acc,check_val_acc,check_test_acc))\n",
    "\n",
    "# np.random.shuffle(ref_indices)\n",
    "# at_tr_data_tensor=torch.from_numpy(ref_data[ref_indices[:target_tr_len]]).type(torch.FloatTensor)\n",
    "# at_tr_label_tensor=torch.from_numpy(ref_label[ref_indices[:target_tr_len]]).type(torch.LongTensor)\n",
    "\n",
    "for epoch in range(attack_epochs):\n",
    "    if epoch in at_schedule:\n",
    "        for param_group in attack_optimizer.param_groups:\n",
    "            param_group['lr'] *= at_gamma\n",
    "            print('Epoch %d Local lr %f'%(epoch,param_group['lr']))\n",
    "\n",
    "    at_loss, at_acc = attack_bb(mia_train_members_data_tensor, mia_train_members_label_tensor,\n",
    "                                mia_train_nonmembers_data_tensor, mia_train_nonmembers_label_tensor,\n",
    "                                best_model, attack_model, criterion, criterion_classifier, attack_criterion, best_opt,\n",
    "                                attack_optimizer, epoch, use_cuda, is_train=True, batch_size=64)\n",
    "\n",
    "    at_val_loss, at_val_acc = attack_bb(mia_val_members_data_tensor, mia_val_members_label_tensor,\n",
    "                                        mia_val_nonmembers_data_tensor, mia_val_nonmembers_label_tensor,\n",
    "                                        best_model, attack_model, criterion, criterion_classifier, attack_criterion, best_opt,\n",
    "                                        attack_optimizer, epoch, use_cuda, is_train=False, batch_size=64)\n",
    "\n",
    "    is_best = at_val_acc > best_at_val_acc\n",
    "\n",
    "    if is_best:\n",
    "        at_test_loss, best_at_test_acc = attack_bb(mia_test_members_data_tensor, mia_test_members_label_tensor,\n",
    "                                                   mia_test_nonmembers_data_tensor, mia_test_nonmembers_label_tensor, \n",
    "                                                   best_model, attack_model, criterion, criterion_classifier, attack_criterion, best_opt,\n",
    "                                                   attack_optimizer, epoch, use_cuda, is_train=False, batch_size=64)\n",
    "\n",
    "\n",
    "    best_at_val_acc = max(best_at_val_acc, at_val_acc)\n",
    "    \n",
    "    print('protected_model | epoch %d | attack val acc %.4f | best val acc %.4f | test acc %.4f'%(epoch, at_val_acc, best_at_val_acc, best_at_test_acc) )\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
